\capitulo{4}{Técnicas y herramientas}

En esta sección se presentarán las distintas herramientas y recursos que se han utilizado para la realización del proyecto. 
% aquí debería venir una pequeña enumeración de lenguajes, bibliotecas, APIs, BBDD...


\section{Técnicas}
En el presente proyecto, se han utilizado diversas técnicas para llevar a cabo el análisis y cálculo del índice de impacto de publicaciones científicas. Estas técnicas han sido seleccionadas con el objetivo de obtener resultados precisos y confiables, y de cubrir las necesidades específicas de este estudio.

\subsection{Web-Scraping}
Es la práctica de recopilar datos a través de un programa que interactúe con una API~\cite{mitchell2018}. Más concretamente, un programa automatizado compuesto por \textit{queries} que realizan solicitudes HTTP para adquirir recursos de un sitio web específico. Esta solicitud se puede formatear en una URL que contenga una consulta GET o en un mensaje HTTP que contenga una consulta POST~\cite{zhao2017}. Una vez que la petición es exitosamente recibida y manejada por el sitio web seleccionado, el recurso requerido será extraído y luego devuelto al programa de \textit{web-scraping} específico.

El uso \textit{web-scraping} resulta ser una técnica efectiva en este proyecto debido a que permite obtener datos de las distintas fuentes web de manera automatizada y eficiente. La información obtenida a través de esta técnica es esencial para calcular el índice de impacto (JCR). Además, el \textit{web-scraping} permite obtener grandes cantidades de información en un corto período de tiempo, lo que resulta muy útil en proyectos en el que se requiere una gran cantidad de datos.


\section{Lenguaje de programación}
En primer lugar, nos planteamos la cuestión del lenguaje o lenguajes de programación más adecuados para nuestro objetivo. Las listas de popularidad actuales nos muestran dos entornos ganadores para proyectos de \textit{web-scraping}: Python y Javascript.

Aunque ambos lenguajes son altamente capaces para nuestro proyecto, la enorme base de conocimientos y la diversidad de herramientas creadas en el universo \textbf{Python} decanta la balanza hacia ese lado. Quizás JavaScript permita mejores resultados usando la gestión de memoria en \textit{requests} simultáneas, pero a costa de un código más oscuro y difícil de mantener. Aunque JavaScript cuenta con un gran repertorio de paquetes Node.JS como utilidades de \textit{web-scraping}, en el entorno Python es difícil imaginar una tarea para la que no se haya escrito una (o más) herramientas que resuelvan eficazmente nuestro problema. 

Por otro lado, la comunidad de programadores de Python es inmensa y su creciente popularidad facilita el hallazgo de soluciones rápidamente, tanto en los foros como en la extensa documentación con la que cuenta. Aunque encontramos un rendimiento ligeramente inferior a otros lenguajes en ciertas búsquedas, es el precio a pagar por el tipado dinámico. 

Como valor añadido, Python es fácil de mantener cuando necesitamos adaptar nuestro código a las cambiantes estructuras de las páginas web. Además, sus reconocidas herramientas de análisis de datos nos permiten continuar en el mismo entorno, sin necesidad de buscar alternativas para afrontar tareas relacionadas con la \textit{data science}.

\section{Bibliotecas}
A lo largo del proyecto se ha recurrido a diversas bibliotecas de \textbf{Python}. A continuación, se presenta brevemente cada una de ellas.

\subsection{Scholarly}
Se trata de una biblioteca de Python que permite acceder a los datos de Google Scholar de manera fácil y rápida. La biblioteca proporciona una interfaz sencilla para buscar y recuperar información sobre artículos, autores y revistas en Google Scholar, incluyendo metadatos, citas y otra información relacionada.

Sin embargo, esta biblioteca ha terminado siendo descartada para este proyecto. A diferencia de otras técnicas de \textit{web-scraping}, Scholarly no está diseñada para extraer grandes cantidades de datos de Google Scholar. La biblioteca tiene una serie de limitaciones en cuanto a la cantidad de datos que se pueden recolectar, ya que está diseñada para ser utilizada en investigaciones científicas y no para la extracción masiva de datos. Además, Scholarly está diseñada para respetar los términos de servicio de Google Scholar y no violar la política de uso de la plataforma, por lo que no se recomienda su uso para recolectar grandes cantidades de datos.

En resumen Scholarly es una herramienta útil para acceder a información científica y académica de manera rápida y sencilla, pero no esta diseñada para extraer grandes cantidades de datos.

\subsection{Beautiful Soup}
Beautiful Soup es una biblioteca de Python extremadamente útil para la extracción de datos de páginas HTML o XML. Actualmente se encuentra en su versión 4.8.1. En el siguiente \href{https://beautiful-soup-4.readthedocs.io/en/latest/}{enlace} se puede acceder a la documentación de su página oficial. 

Esta biblioteca nos proporciona numerosos módulos para navegar a través de las páginas web y para extraer fácilmente su contenido. Puesto que gran parte del proyecto se basa en el uso de \textit{web-scraping}, se ha hecho uso intensivo de la misma para extraer los principales datos de los artículos científicos que posteriormente conformarán la BBDD del proyecto.

De Beautiful Soup hay que destacar su facilidad de uso y la amplia documentación que aporta. En su página web nos aconseja utilizar el analizador \textit{lxml}, que proporciona al entorno Python la disponibilidad de las bibliotecas \textit{libxml2} y \textit{libxslt}. Allí mismo, se anima incluso a utilizar aisladamente este parser cuando el tiempo de respuesta sea una cuestión crítica. En nuestro caso, las facilidades que proporciona Beautiful Soup justifican ampliamente su uso, aunque la  rapidez de resultados no iguale la de la utilización aislada de los analizadores sobre los que trabaja.
 

\subsection{Habanero}
Para poder hacer uso de la \textbf{API de Crossref}, se han explorado diversas alternativas. De entre todas las bibliotecas de Python se ha seleccionado Habanero, ya que es una biblioteca muy fácil de usar y está en constante actualización.

Esta biblioteca está diseñada para facilitar el acceso a las bases de datos de revistas científicas y a otras relacionadas con el ámbito académico. Ofrece una interfaz simple para recuperar información utilizando los protocolos y las API de diferentes bases de datos, incluyendo JSTOR, Unpaywall, Crossref, DataCite, etc. Además, Habanero es compatible con las normas de Open Access, lo que permite a los usuarios acceder a contenido científico gratuito y de libre acceso.

\subsection{Selenium}

Selenium es una biblioteca de Python de automatización de navegadores web que se utiliza para controlar el comportamiento de un navegador en tiempo real. Es una herramienta muy poderosa para realizar \textit{web scraping} de sitios web, especialmente en aquellos casos en que se necesita interactuar con la página web como si lo hiciera un usuario humano.

A diferencia de otras bibliotecas de \textit{web scraping}, Selenium permite simular la navegación humana en la web, permitiendo realizar búsquedas, hacer clic en botones, llenar formularios, entre otras acciones. Esto es muy útil cuando se trata de sitios web que tienen medidas de seguridad para evitar el \textit{web scraping} y requieren de interacción humana.


\section{Bases de datos}

Antes de su compra por Oracle Corporation (2010), MySQL era la aplicación de base de datos más popular de código abierto para la programación web. En el momento actual, el software libre nos ofrece dos soluciones ampliamente contrastadas en gestores de bases de datos relacionales: MariaDB y PostgreSQL. Necesitaremos una herramienta de este tipo para organizar los datos recolectados. La estructura tabular de la información nos permite aplicar sobre ella el lenguaje de interrogación SQL.

En realidad, MariaDB es una bifurcación de MySQL nacida para garantizar la supervivencia del proyecto como código abierto. Hoy en día, MariaDB es altamente compatible con MySQL e incluso superior en sus últimas versiones (10.1.1), ya que la comunidad ha ido añadiendo nuevas características al proyecto original. Por otro lado, aunque no está tan extendido como MySQL, PostgreSQL es posiblemente el gestor de bases de datos de código abierto más sólido y potente a día de hoy.


\tablaSmall{Comparativa entre MariaDB y PostgreSQL}{l l}{comparativabbdd}
{ MariaDB & PostgreSQL\\}{ 
No totalmente compatible con SQL & Compatible con SQL estándar\\
Soporte para tipos de datos estándar SQL & Soporta además tipos avanzados\\
Tipos de datos flexibles & Tipos de datos estrictos \\
Tamaño pequeño de base de datos & Tamaño grande de base de datos \\
Sin soporte directo para JSON  & Soporte directo de JSON \\
No índices parciales & Índices parciales \\
No soporte para web dinámica & Soporta sitios web dinámicos\\
} 


\subsection{PostgreSQL}
Finalmente se opta por \href{https://www.postgresql.org/}{PostgreSQL} (también denominado Postgres). Pese a que ambas opciones son muy similares, se ha elegido esta última debido a que ya se ha trabajado con ella anteriormente. Además, PostgreSQL posee una sólida reputación por su arquitectura comprobada, confiabilidad, integridad de datos, conjunto sólido de características, extensibilidad y la dedicación de la comunidad de código abierto detrás del software para ofrecer soluciones innovadoras y de rendimiento constante~\cite{Postgresql.org}.


Al igual que MariaDB, es un sistema de gestión de bases de datos relacional de código abierto. Así pues, está dirigido y desarrollado por una comunidad altruista de desarrolladores (PostgreSQL Global Development Group).
PostgreSQL utiliza y amplía el lenguaje SQL combinado con muchas características que almacenan y escalan de forma segura las cargas de trabajo de datos más complicadas~\cite{Postgresql.org}. 
Su última versión y la usada para el proyecto es la versión 15.1 lanzada el 10 de noviembre de este año.



\section{APIs}
A lo largo del proyecto se ha recurrido a varias APIs diferentes, todas ellas de acceso gratuito.

\subsection{Google Scholar}
Se trata de una de las principales herramientas que ofrece Google a los investigadores. \href{https://scholar.google.com/}{Google Scholar} es, fundamentalmente, un buscador de contenido y bibliografía científica que permite localizar artículos de revistas especializadas ordenados por relevancia en función de las palabras clave introducidas en el buscador. También se puede  filtrar la información en función de su fecha de publicación, idioma o número de citas.


\subsection{Crossref}

 Crossref es una organización sin ánimo de lucro que provee de una \href{https://www.crossref.org/}{herramienta} que facilita el acceso a la información de los artículos científicos a partir de su DOI\footnote{El DOI (\textit{Digital Object Identifier}) es el acrónimo con el que se conoce al identificador inequívoco y persistente de un objeto digital (como un artículo de revista, un libro electrónico, una imagen, etc...). Se trata de un enlace permanente al contenido electrónico de dicho objeto. Por lo general, un DOI tiene forma de código alfanumérico.}.

Como ya se ha mencionado previamente, en base a la información de los DOIs la agencia de registro CrossRef es capaz de proporcionarnos aplicaciones útiles para hacer nuestro flujo de investigación más sencillo.
Se trata de una asociación sin ánimo de lucro de editoriales científicas que no solo facilita el registro de DOIs a las editoriales, sino que también ofrece servicios y aplicaciones para el personal investigador que tienen como base estos códigos.

Los DOIs que CrossRef almacena van acompañados de información que refleja las cualidades básicas de una publicación científica. Me estoy refiriendo a datos como títulos, abstracts, palabras clave, autores…

CrossRef Metadata Search hace posible obtener toda esta información al instante con tan solo proporcionar el DOI asociado a una publicación, o al contrario, obtener el DOI de la publicación con tan solo introducir algunos de estos datos en su buscador.








