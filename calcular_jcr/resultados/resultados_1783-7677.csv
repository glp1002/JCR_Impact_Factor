DOI,Título,Autores,Año,Citas,Revista
10.1007/s12193-013-0131-2,Switching Wizard of Oz for the online evaluation of backchannel behavior,Ronald Poppe and Mark ter Maat and Dirk Heylen,2013,1,Journal on Multimodal User Interfaces
10.1007/bf02884430,Realtime and accurate musical control of expression in singing synthesis,Nicolas D’Alessandro and Pascale Woodruff and Yohann Fabre and Thierry Dutoit and Sylvain Le Beux and Boris Doval and Christophe d’Alessandro,2007,8,Journal on Multimodal User Interfaces
10.1007/s12193-015-0180-9,Orientation estimation in modern wearables with visual feature tracking,László Kundra and Péter Ekler and Hassan Charaf,2015,1,Journal on Multimodal User Interfaces
10.1007/s12193-011-0074-4,Art at the edges of materiality,Ina Conradi,2012,1,Journal on Multimodal User Interfaces
10.1007/s12193-017-0252-0,Studying gesture-based interaction on a mobile augmented reality application for co-design activity,Leman Figen Gül,2018,7,Journal on Multimodal User Interfaces
10.1007/s12193-008-0008-y,Multimodal identification and localization of users in a smart environment,Albert Ali Salah and Ramon Morros and Jordi Luque and Carlos Segura and Javier Hernando and Onkar Ambekar and Ben Schouten and Eric Pauwels,2008,13,Journal on Multimodal User Interfaces
10.1007/s12193-011-0058-4,Exploiting on-the-fly interpretation to design technical documents in a mobile context,Sébastien Macé and Eric Anquetil,2011,0,Journal on Multimodal User Interfaces
10.1007/bf02884431,An agent based multicultural tour guide system with nonverbal user interface,Hung-Hsuan Huang and Kateryna Tarasenko and Toyoaki Nishida and Aleksandra Cerekovic and Vjekoslav Levacic and Goranka Zoric and Igor S. Pandzic and Yukiko Nakano,2007,2,Journal on Multimodal User Interfaces
10.1007/s12193-018-0259-1,A natural interface based on intention prediction for semi-autonomous micromanipulation,Laura Cohen and Mohamed Chetouani and Stéphane Régnier and Sinan Haliyo,2018,3,Journal on Multimodal User Interfaces
10.1007/s12193-018-0275-1,Speech communication integrated with other modalities,Alexey Karpov and Iosif Mporas,2018,0,Journal on Multimodal User Interfaces
10.1007/s12193-014-0152-5,From multimodal analysis to real-time interactions with virtual agents,Ronald Poppe and Ronald Böck and Francesca Bonin and Nick Campbell and Iwan de Kok and David Traum,2014,2,Journal on Multimodal User Interfaces
10.1007/s12193-009-0017-5,Real-time motion attention and expressive gesture interfaces,Matei Mancas and Donald Glowinski and Gualtiero Volpe and Antonio Camurri and Pierre Bretéché and Jonathan Demeyer and Thierry Ravet and Paolo Coletta,2008,3,Journal on Multimodal User Interfaces
10.1007/s12193-011-0071-7,Affective transfer computing model based on attenuation emotion mechanism,Zhiguo Shi and Junming Wei and Zhiliang Wang and Jun Tu and Qiao Zhang,2012,8,Journal on Multimodal User Interfaces
10.1007/s12193-020-00360-w,Correction to: The Augmented Movement Platform For Embodied Learning (AMPEL): development and reliability,Lousin Moumdjian and Thomas Vervust and Joren Six and Ivan Schepers and Micheline Lesaffre and Peter Feys and Marc Leman,2021,0,Journal on Multimodal User Interfaces
10.1007/s12193-017-0241-3,Extraction of texture and geometrical features from informative facial regions for sign language recognition,Sunil Kumar and M. K. Bhuyan and Biplab Ketan Chakraborty,2017,4,Journal on Multimodal User Interfaces
10.1007/s12193-022-00387-1,A review on communication cues for augmented reality based remote guidance,Weidong Huang and Mathew Wakefield and Troels Ammitsbøl Rasmussen and Seungwon Kim and Mark Billinghurst,2022,2,Journal on Multimodal User Interfaces
10.1007/s12193-011-0079-z,Interactive sonification of synchronisation of motoric behaviour in social active listening to music with mobile devices,Giovanna Varni and Gaël Dubus and Sami Oksanen and Gualtiero Volpe and Marco Fabiani and Roberto Bresin and Jari Kleimola and Vesa Välimäki and Antonio Camurri,2012,18,Journal on Multimodal User Interfaces
10.1007/bf02884429,Synthesis of expressive facial animations: A multimodal caricatural mirror,Olivier Martin and Irene Kotsia and Ioannis Pitas and Arman Savran and Jordi Adell and Ana Huerta and Raphael Sebbe,2007,0,Journal on Multimodal User Interfaces
10.1007/s12193-019-00317-8,Focused Audification and the optimization of its parameters,Katharina Groß-Vogt and Matthias Frank and Robert Höldrich,2020,2,Journal on Multimodal User Interfaces
10.1007/s12193-020-00346-8,Multimodal interfaces and communication cues for remote collaboration,Seungwon Kim and Mark Billinghurst and Kangsoo Kim,2020,16,Journal on Multimodal User Interfaces
10.1007/s12193-014-0159-y,A comparative study of game mechanics and control laws for an adaptive physiological game,Avinash Parnandi and Ricardo Gutierrez-Osuna,2015,24,Journal on Multimodal User Interfaces
10.1007/s12193-022-00388-0,A survey of challenges and methods for Quality of Experience assessment of interactive VR applications,Sara Vlahovic and Mirko Suznjevic and Lea Skorin-Kapov,2022,5,Journal on Multimodal User Interfaces
10.1007/bf02884427,How really effective are multimodal hints in enhancing visual target spotting? Some evidence from a usability study,Suzanne Kieffer and Noëlle Carbonell,2007,3,Journal on Multimodal User Interfaces
10.1007/s12193-021-00384-w,Importance of force feedback for following uneven virtual paths with a stylus,Federico Fontana and Francesco Muzzolini and Davide Rocchesso,2022,0,Journal on Multimodal User Interfaces
10.1007/s12193-021-00381-z,The effect of eye movement sonification on visual search patterns and anticipation in novices,Maryam Khalaji and Mahin Aghdaei and Alireza Farsi and Alessandro Piras,2022,0,Journal on Multimodal User Interfaces
10.1007/s12193-020-00343-x,Exploring interaction techniques for 360 panoramas inside a 3D reconstructed scene for mixed reality remote collaboration,Theophilus Teo and Mitchell Norman and Gun A. Lee and Mark Billinghurst and Matt Adcock,2020,11,Journal on Multimodal User Interfaces
10.1007/s12193-020-00363-7,Facial expression and action unit recognition augmented by their dependencies on graph convolutional networks,Jun He and Xiaocui Yu and Bo Sun and Lejun Yu,2021,5,Journal on Multimodal User Interfaces
10.1007/s12193-018-0278-y,Interactive sonification of a fluid dance movement: an exploratory study,Emma Frid and Ludvig Elblaus and Roberto Bresin,2019,2,Journal on Multimodal User Interfaces
10.1007/s12193-013-0126-z,Multimodal interaction: a survey from model driven engineering and mobile perspectives,Nadia Elouali and José Rouillard and Xavier Le Pallec and Jean-Claude Tarby,2013,13,Journal on Multimodal User Interfaces
10.1007/s12193-019-00304-z,Spatial-temporal dynamic hand gesture recognition via hybrid deep learning model,Jinghua Li and Huarui Huai and Junbin Gao and Dehui Kong and Lichun Wang,2019,6,Journal on Multimodal User Interfaces
10.1007/s12193-014-0151-6,Empathetic video clip experience through timely multimodal interaction,Myunghee Lee and Gerard Jounghyun Kim,2014,3,Journal on Multimodal User Interfaces
10.1007/s12193-011-0086-0,Naturalness influences the perceived usability and pleasantness of an interface’s sonic feedback,Patrick Susini and Nicolas Misdariis and Guillaume Lemaitre and Olivier Houix,2012,12,Journal on Multimodal User Interfaces
10.1007/s12193-012-0110-z,A multimodal study of answers to disruptions,Brigitte Bigi and Cristel Portes and Agnès Steuckardt and Marion Tellier,2013,3,Journal on Multimodal User Interfaces
10.1007/s12193-013-0128-x,Facial expression-based affective speech translation,Éva Székely and Ingmar Steiner and Zeeshan Ahmed and Julie Carson-Berndsen,2014,6,Journal on Multimodal User Interfaces
10.1007/s12193-015-0177-4,Typing performance evaluation with multimodal soft keyboard completely integrated in commercial mobile devices,Byungkil Han and Kwangtaek Kim,2015,10,Journal on Multimodal User Interfaces
10.1007/s12193-021-00385-9,Preliminary assessment of a multimodal electric-powered wheelchair simulator for training of activities of daily living,Felipe R. Martins and Eduardo L. M. Naves and Yann Morère and Angela A. R. de Sá,2022,2,Journal on Multimodal User Interfaces
10.1007/bf02910053,Preface,Benoit Macq,2007,0,Journal on Multimodal User Interfaces
10.1007/s12193-020-00326-y,Exploring crossmodal perceptual enhancement and integration in a sequence-reproducing task with cognitive priming,Feng Feng and Puhong Li and Tony Stockman,2021,1,Journal on Multimodal User Interfaces
10.1007/s12193-012-0113-9,A kinect-based interface to animate virtual characters,Andrea Sanna and Fabrizio Lamberti and Gianluca Paravati and Felipe Domingues Rocha,2013,12,Journal on Multimodal User Interfaces
10.1007/s12193-020-00323-1,Speech and web-based technology to enhance education for pupils with visual impairment,Jindřich Matoušek and Zdeněk Krňoul and Michal Campr and Zbyněk Zajíc and Zdeněk Hanzlíček and Martin Grůber and Marie Kocurová,2020,5,Journal on Multimodal User Interfaces
10.1007/s12193-018-0289-8,Training doctors’ social skills to break bad news: evaluation of the impact of virtual environment displays on the sense of presence,Magalie Ochs and Daniel Mestre and Grégoire de Montcheuil and Jean-Marie Pergandi and Jorane Saubesty and Evelyne Lombardo and Daniel Francon and Philippe Blache,2019,19,Journal on Multimodal User Interfaces
10.1007/s12193-019-00305-y,GG Interaction: a gaze–grasp pose interaction for 3D virtual object selection,Kunhee Ryu and Joong-Jae Lee and Jung-Min Park,2019,10,Journal on Multimodal User Interfaces
10.1007/s12193-016-0227-6,Non-speech voice for sonic interaction: a catalogue,Alan Del Piccolo and Davide Rocchesso,2017,1,Journal on Multimodal User Interfaces
10.1007/s12193-019-00314-x,Comparison of spatial and temporal interaction techniques for 3D audio trajectory authoring,Justin D. Mathew and Stéphane Huot and Brian F. G. Katz,2020,3,Journal on Multimodal User Interfaces
10.1007/s12193-021-00378-8,Combining audio and visual displays to highlight temporal and spatial seismic patterns,Arthur Paté and Gaspard Farge and Benjamin K. Holtzman and Anna C. Barth and Piero Poli and Lapo Boschi and Leif Karlstrom,2022,1,Journal on Multimodal User Interfaces
10.1007/s12193-018-0288-9,Virtual discrete trial training for teacher trainees,Berglind Sveinbjörnsdóttir and Snorri Hjörvar Jóhannsson and Júlía Oddsdóttir and Tinna Þuríður Sigurðardóttir and Gunnar Ingi Valdimarsson and Hannes Högni Vilhjálmsson,2019,9,Journal on Multimodal User Interfaces
10.1007/s12193-020-00328-w,Improving robot’s perception of uncertain spatial descriptors in navigational instructions by evaluating influential gesture notions,M. A. Viraj J. Muthugala and P. H. D. Arjuna S. Srimal and A. G. Buddhika P. Jayasekara,2021,3,Journal on Multimodal User Interfaces
10.1007/s12193-010-0049-x,Browsing a dance video collection: dance analysis and interface design,Damien Tardieu and Xavier Siebert and Barbara Mazzarino and Ricardo Chessini and Julien Dubois and Stéphane Dupont and Giovanna Varni and Alexandra Visentin,2010,6,Journal on Multimodal User Interfaces
10.1007/s12193-013-0145-9,Feedback facial expressions and emotions,Costanza Navarretta,2014,3,Journal on Multimodal User Interfaces
10.1007/s12193-019-00309-8,Gaze-based interactions in the cockpit of the future: a survey,David Rudi and Peter Kiefer and Ioannis Giannopoulos and Martin Raubal,2020,6,Journal on Multimodal User Interfaces
10.1007/s12193-015-0186-3,A framework towards expressive speech analysis and synthesis with preliminary results,Spyros Raptis and Sotiris Karabetsos and Aimilios Chalamandaris and Pirros Tsiakoulis,2015,1,Journal on Multimodal User Interfaces
10.1007/s12193-008-0002-4,"Experiences with digital pen, keyboard and mouse usability",Beryl Plimmer,2008,7,Journal on Multimodal User Interfaces
10.1007/s12193-018-0280-4,Musical Vision: an interactive bio-inspired sonification tool to convert images into music,Antonio Polo and Xavier Sevillano,2019,10,Journal on Multimodal User Interfaces
10.1007/s12193-016-0220-0,Sonification and music as support to the communication of alcohol-related health risks to young people,Bartlomiej P. Walus and Sandra Pauletto and Amanda Mason-Jones,2016,6,Journal on Multimodal User Interfaces
10.1007/s12193-011-0088-y,Measuring the Quality of Service and Quality of Experience of multimodal human–machine interaction,Ina Wechsung and Klaus-Peter Engelbrecht and Christine Kühnel and Sebastian Möller and Benjamin Weiss,2012,28,Journal on Multimodal User Interfaces
10.1007/s12193-020-00349-5,Evaluation of avatar and voice transform in programming e-learning lectures,Rex Hsieh and Hisashi Sato,2021,5,Journal on Multimodal User Interfaces
10.1007/s12193-015-0175-6,Combining modality-specific extreme learning machines for emotion recognition in the wild,Heysem Kaya and Albert Ali Salah,2016,23,Journal on Multimodal User Interfaces
10.1007/s12193-012-0097-5,A three-component framework for empathic technologies to augment human interaction,Joris H. Janssen,2012,24,Journal on Multimodal User Interfaces
10.1007/bf02884432,Multimodal signal processing and interaction for a driving simulator: Component-based architecture,Alexandre Benoit and Laurent Bonnaud and Alice Caplier and Frédéric Jourde and Laurence Nigay and Marcos Serrano and Ioannis Damousis and Dimitrios Tzovaras and Jean-Yves Lionel Lawson,2007,17,Journal on Multimodal User Interfaces
10.1007/s12193-016-0213-z,Emotion recognition in the wild,Abhinav Dhall and Roland Goecke and Tom Gedeon and Nicu Sebe,2016,8,Journal on Multimodal User Interfaces
10.1007/s12193-016-0224-9,Gamified music improvisation with BilliArT: a multimodal installation with balls,T. Vets and L. Nijs and M. Lesaffre and B. Moens and F. Bressan and P. Colpaert and P. Lambert and R. Van de Walle and M. Leman,2017,7,Journal on Multimodal User Interfaces
10.1007/s12193-010-0042-4,A wizard of oz component-based approach for rapidly prototyping and testing input multimodal interfaces,Marcos Serrano and Laurence Nigay,2010,9,Journal on Multimodal User Interfaces
10.1007/s12193-010-0051-3,Elckerlyc,Herwin van Welbergen and Dennis Reidsma and Zsófia M. Ruttkay and Job Zwiers,2009,41,Journal on Multimodal User Interfaces
10.1007/s12193-020-00325-z,fNIRS-based classification of mind-wandering with personalized window selection for multimodal learning interfaces,Ruixue Liu and Erin Walker and Leah Friedman and Catherine M. Arrington and Erin T. Solovey,2021,4,Journal on Multimodal User Interfaces
10.1007/s12193-012-0093-9,A generic framework for the inference of user states in human computer interaction,Stefan Scherer and Michael Glodek and Georg Layher and Martin Schels and Miriam Schmidt and Tobias Brosch and Stephan Tschechne and Friedhelm Schwenker and Heiko Neumann and Günther Palm,2012,28,Journal on Multimodal User Interfaces
10.1007/s12193-018-0272-4,Personalisation and automation in a virtual conversation skills tutor for children with autism,Marissa Milne and Parimala Raghavendra and Richard Leibbrandt and David Martin Ward Powers,2018,9,Journal on Multimodal User Interfaces
10.1007/s12193-021-00375-x,Interactive exploration of a hierarchical spider web structure with sound,Isabelle Su and Ian Hattwick and Christine Southworth and Evan Ziporyn and Ally Bisshop and Roland Mühlethaler and Tomás Saraceno and Markus J. Buehler,2022,1,Journal on Multimodal User Interfaces
10.1007/s12193-018-0277-z,Model driven approach for adapting user interfaces to the context of accessibility: case of visually impaired users,Yousra Bendaly Hlaoui and Lamia Zouhaier and Leila Ben Ayed,2019,4,Journal on Multimodal User Interfaces
10.1007/s12193-013-0137-9,Collaborative navigation of visually impaired,Jan Balata and Jakub Franc and Zdenek Mikovec and Pavel Slavik,2014,18,Journal on Multimodal User Interfaces
10.1007/s12193-011-0070-8,Evaluation of an image-based talking head with realistic facial expression and head motion,Kang Liu and Joern Ostermann,2012,1,Journal on Multimodal User Interfaces
10.1007/s12193-020-00329-9,Effects of personality traits on user trust in human–machine collaborations,Jianlong Zhou and Simon Luo and Fang Chen,2020,13,Journal on Multimodal User Interfaces
10.1007/s12193-008-0003-3,Assessment of a multimodal interaction and rendering system against established design principles,Robbie Schaefer and Wolfgang Mueller,2008,2,Journal on Multimodal User Interfaces
10.1007/s12193-009-0014-8,Feeling what you hear: task-irrelevant sounds modulate tactile perception delivered via a touch screen,Ju-Hwan Lee and Charles Spence,2008,11,Journal on Multimodal User Interfaces
10.1007/s12193-012-0103-y,The influence of facial interface design on dynamic emotional recognition,Michel Dubois and Damien Dupré and Jean-Michel Adam and Anna Tcherkassof and Nadine Mandran and Brigitte Meillon,2013,1,Journal on Multimodal User Interfaces
10.1007/s12193-015-0183-6,Socio-cognitive gamification: general framework for educational games,Luca Szegletes and Mate Koles and Bertalan Forstner,2015,10,Journal on Multimodal User Interfaces
10.1007/s12193-020-00354-8,The Augmented Movement Platform For Embodied Learning (AMPEL): development and reliability,Lousin Moumdjian and Thomas Vervust and Joren Six and Ivan Schepers and Micheline Lesaffre and Peter Feys and Marc Leman,2021,2,Journal on Multimodal User Interfaces
10.1007/s12193-010-0056-y,Multi-modal movement reconstruction for stroke rehabilitation and performance assessment,Benoit Caby and Julien Stamatakis and Patrice Laloux and Benoit Macq and Yves Vandermeeren,2011,3,Journal on Multimodal User Interfaces
10.1007/s12193-016-0225-8,On the robustness of upper limits for circular auditory motion perception,Cédric Camier and Julien Boissinot and Catherine Guastavino,2016,2,Journal on Multimodal User Interfaces
10.1007/s12193-016-0214-y,Subjective HRTF evaluations for obtaining global similarity metrics of assessors and assessees,Areti Andreopoulou and Brian F. G. Katz,2016,12,Journal on Multimodal User Interfaces
10.1007/s12193-015-0202-7,Revisiting the EmotiW challenge: how wild is it really?,Markus Kächele and Martin Schels and Sascha Meudt and Günther Palm and Friedhelm Schwenker,2016,15,Journal on Multimodal User Interfaces
10.1007/s12193-009-0034-4,Multi-touch user interface evaluation for 3D object manipulation on mobile devices,Donato Fiorella and Andrea Sanna and Fabrizio Lamberti,2010,21,Journal on Multimodal User Interfaces
10.1007/s12193-013-0120-5,The W3C multimodal architecture and interfaces standard,Deborah A. Dahl,2013,21,Journal on Multimodal User Interfaces
10.1007/s12193-016-0229-4,Action recognition based on binary patterns of action-history and histogram of oriented gradient,Md. Atiqur Rahman Ahad and Md. Nazmul Islam and Israt Jahan,2016,18,Journal on Multimodal User Interfaces
10.1007/s12193-021-00374-y,Correction to: A gaze-based interactive system to explore artwork imagery,Piercarlo Dondi and Marco Porta and Angelo Donvito and Giovanni Volpe,2022,0,Journal on Multimodal User Interfaces
10.1007/s12193-009-0022-8,Communication of musical expression by means of mobile robot gestures,Birgitta Burger and Roberto Bresin,2010,5,Journal on Multimodal User Interfaces
10.1007/s12193-016-0219-6,Critical review of the book “Gaze in Human–Robot Communication”,Gérard Bailly,2017,0,Journal on Multimodal User Interfaces
10.1007/s12193-011-0069-1,Multi-modal musical environments for mixed-reality performance,Robert Hamilton and Juan-Pablo Caceres and Chryssie Nanou and Chris Platz,2011,2,Journal on Multimodal User Interfaces
10.1007/s12193-020-00320-4,A multimodal auditory equal-loudness comparison of air and bone conducted sounds,Rafael N. C. Patrick and Tomasz R. Letowski and Maranda E. McBride,2020,1,Journal on Multimodal User Interfaces
10.1007/s12193-011-0089-x,Sound/tracks: artistic real-time sonification of train journeys,Peter Knees and Tim Pohle and Gerhard Widmer,2012,1,Journal on Multimodal User Interfaces
10.1007/s12193-013-0132-1,Iterative perceptual learning for social behavior synthesis,Iwan de Kok and Ronald Poppe and Dirk Heylen,2014,0,Journal on Multimodal User Interfaces
10.1007/s12193-022-00395-1,Gesture-based guidance for navigation in virtual environments,Inam Ur Rehman and Sehat Ullah and Numan Ali and Ihsan Rabbi and Riaz Ullah Khan,2022,0,Journal on Multimodal User Interfaces
10.1007/s12193-012-0090-z,Generating context-sensitive ECA responses to user barge-in interruptions,Nigel Crook and Debora Field and Cameron Smith and Sue Harding and Stephen Pulman and Marc Cavazza and Daniel Charlton and Roger Moore and Johan Boye,2012,7,Journal on Multimodal User Interfaces
10.1007/s12193-016-0226-7,Advances in auditory display research,Brian F. G. Katz and Georgios Marentakis,2016,2,Journal on Multimodal User Interfaces
10.1007/s12193-023-00401-0,Personality trait estimation in group discussions using multimodal analysis and speaker embedding,Candy Olivia Mawalim and Shogo Okada and Yukiko I. Nakano and Masashi Unoki,2023,0,Journal on Multimodal User Interfaces
10.1007/s12193-017-0254-y,Evaluation of hand-foot coordinated quadruped interaction for mobile applications,Youngwon Kim and Euijai Ahn and Gerard Jounghyun Kim,2017,0,Journal on Multimodal User Interfaces
10.1007/s12193-011-0072-6,Expression sequences generator for synthetic emotion,Mingmin Zhang and Xiaojian Zhou and Nan Xiang and Yuyong He and Zhigeng Pan,2012,1,Journal on Multimodal User Interfaces
10.1007/s12193-011-0065-5,Eyes-free environmental awareness for navigation,Dalia El-Shimy and Florian Grond and Adriana Olmos and Jeremy R. Cooperstock,2012,6,Journal on Multimodal User Interfaces
10.1007/s12193-014-0160-5,Multimodal PTSD characterization via the StartleMart game,Christoffer Holmgård and Georgios N. Yannakakis and Héctor P. Martínez and Karen-Inge Karstoft and Henrik Steen Andersen,2015,12,Journal on Multimodal User Interfaces
10.1007/s12193-013-0124-1,MARC: a framework that features emotion models for facial animation during human–computer interaction,M. Courgeon and C. Clavel,2013,24,Journal on Multimodal User Interfaces
10.1007/s12193-014-0171-2,Eye moving behaviors identification for gaze tracking interaction,Qijie Zhao and Xinming Yuan and Dawei Tu and Jianxia Lu,2015,12,Journal on Multimodal User Interfaces
10.1007/s12193-012-0116-6,Multimodal notification framework for elderly and professional in a smart nursing home,Mahmoud Ghorbel and Stéphane Betgé-Brezetz and Marie Pascale Dupont and Guy Bertrand Kamga and Sophie Piekarec and Juliette Reerink and Arnaud Vergnol,2013,5,Journal on Multimodal User Interfaces
10.1007/s12193-012-0091-y,Conceptual analysis of social signals: the importance of clarifying terminology,Marc Mehu and Francesca D’Errico and Dirk Heylen,2012,3,Journal on Multimodal User Interfaces
10.1007/s12193-021-00382-y,Informing the design of a multisensory learning environment for elementary mathematics learning,Luigi F. Cuturi and Giulia Cappagli and Nikoleta Yiannoutsou and Sara Price and Monica Gori,2022,0,Journal on Multimodal User Interfaces
10.1007/s12193-012-0107-7,"The LDOS-PerAff-1 corpus of facial-expression video clips with affective, personality and user-interaction metadata",Marko Tkalčič and Andrej Košir and Jurij Tasič,2013,15,Journal on Multimodal User Interfaces
10.1007/s12193-020-00337-9,Virtual intimacy in human-embodied conversational agent interactions: the influence of multimodality on its perception,Delphine Potdevin and Céline Clavel and Nicolas Sabouret,2021,2,Journal on Multimodal User Interfaces
10.1007/s12193-017-0251-1,Improving collaboration efficiency via diverse networked mobile devices,Franca A. Rupprecht and Georg Kasakow and Jan C. Aurich and Bernd Hamann and Achim Ebert,2018,2,Journal on Multimodal User Interfaces
10.1007/s12193-012-0108-6,D64: a corpus of richly recorded conversational interaction,Catharine Oertel and Fred Cummins and Jens Edlund and Petra Wagner and Nick Campbell,2013,19,Journal on Multimodal User Interfaces
10.1007/s12193-016-0223-x,Prioritizing foreground selection of natural chirp sounds by tempo and spectral centroid,Francesco Tordini and Albert S. Bregman and Jeremy R. Cooperstock,2016,4,Journal on Multimodal User Interfaces
10.1007/s12193-015-0197-0,Investigating the effects of motion-based Kinect game system on user cognition,Akihito Nakai and Aung Pyae and Mika Luimula and Satoshi Hongo and Hannu Vuola and Jouni Smed,2015,12,Journal on Multimodal User Interfaces
10.1007/s12193-015-0199-y,Design space exploration of hardware platforms for interactive low latency movement sonification,Hans-Peter Brückner and Sebastian Lesse and Wolfgang Theimer and Holger Blume,2016,4,Journal on Multimodal User Interfaces
10.1007/s12193-018-0279-x,The SoundBike: musical sonification strategies to enhance cyclists’ spontaneous synchronization to external music,Pieter-Jan Maes and Valerio Lorenzoni and Joren Six,2019,8,Journal on Multimodal User Interfaces
10.1007/s12193-015-0198-z,Evaluating metaphor reification in tangible interfaces,Augusto Celentano and Emmanuel Dubois,2015,1,Journal on Multimodal User Interfaces
10.1007/s12193-009-0027-3,From expressive gesture to sound,Pieter-Jan Maes and Marc Leman and Micheline Lesaffre and Michiel Demey and Dirk Moelants,2010,13,Journal on Multimodal User Interfaces
10.1007/s12193-021-00368-w,An expert-model and machine learning hybrid approach to predicting human-agent negotiation outcomes in varied data,Johnathan Mell and Markus Beissinger and Jonathan Gratch,2021,1,Journal on Multimodal User Interfaces
10.1007/s12193-020-00350-y,Virtual agents as supporting media for scientific presentations,Timothy Bickmore and Everlyne Kimani and Ameneh Shamekhi and Prasanth Murali and Dhaval Parmar and Ha Trinh,2021,4,Journal on Multimodal User Interfaces
10.1007/s12193-011-0078-0,Investigating effects of avatars on primary school children’s affective responses to learning,Yin-Leng Theng and Paye Aung,2012,11,Journal on Multimodal User Interfaces
10.1007/s12193-011-0084-2,An embodied music cognition approach to multilevel interactive sonification,Nuno Diniz and Pieter Coussement and Alexander Deweppe and Michiel Demey and Marc Leman,2012,10,Journal on Multimodal User Interfaces
10.1007/s12193-011-0085-1,Evaluation of four models for the sonification of elite rowing,Gaël Dubus,2012,21,Journal on Multimodal User Interfaces
10.1007/s12193-010-0048-y,Generating distinctive behavior for Embodied Conversational Agents,Maurizio Mancini and Catherine Pelachaud,2009,8,Journal on Multimodal User Interfaces
10.1007/s12193-009-0029-1,Investigating shared attention with a virtual agent using a gaze-based interface,Christopher Peters and Stylianos Asteriadis and Kostas Karpouzis,2010,37,Journal on Multimodal User Interfaces
10.1007/s12193-010-0044-2,Interactive design of multimodal user interfaces,Werner A. König and Roman Rädle and Harald Reiterer,2010,21,Journal on Multimodal User Interfaces
10.1007/s12193-017-0242-2,Multi-modal user interface combining eye tracking and hand gesture recognition,Hansol Kim and Kun Ha Suh and Eui Chul Lee,2017,4,Journal on Multimodal User Interfaces
10.1007/s12193-011-0062-8,Knowing by ear: leveraging human attention abilities in interaction design,Saskia Bakker and Elise van den Hoven and Berry Eggen,2012,15,Journal on Multimodal User Interfaces
10.1007/s12193-012-0092-x,Towards a conceptual framework of research on social signal processing,Paul M. Brunet and Roderick Cowie,2012,8,Journal on Multimodal User Interfaces
10.1007/s12193-020-00347-7,Multimodal analysis of personality traits on videos of self-presentation and induced behavior,Dersu Giritlioğlu and Burak Mandira and Selim Firat Yilmaz and Can Ufuk Ertenli and Berhan Faruk Akgür and Merve Kınıklıoğlu and Aslı Gül Kurt and Emre Mutlu and Şeref Can Gürel and Hamdi Dibeklioğlu,2021,4,Journal on Multimodal User Interfaces
10.1007/s12193-020-00353-9,Words of encouragement: how praise delivered by a social robot changes children’s mindset for learning,Daniel P. Davison and Frances M. Wijnen and Vicky Charisi and Jan van der Meij and Dennis Reidsma and Vanessa Evers,2021,10,Journal on Multimodal User Interfaces
10.1007/s12193-021-00386-8,Combining haptics and inertial motion capture to enhance remote control of a dual-arm robot,Vicent Girbés-Juan and Vinicius Schettino and Luis Gracia and J. Ernesto Solanes and Yiannis Demiris and Josep Tornero,2022,4,Journal on Multimodal User Interfaces
10.1007/s12193-013-0138-8,Designing and evaluating a workstation in real and virtual environment: toward virtual reality based ergonomic design sessions,Charles Pontonnier and Georges Dumont and Asfhin Samani and Pascal Madeleine and Marwan Badawi,2014,31,Journal on Multimodal User Interfaces
10.1007/s12193-009-0026-4,HMM modeling of user engagement in advice-giving dialogues,Nicole Novielli,2010,7,Journal on Multimodal User Interfaces
10.1007/s12193-018-0263-5,Landmark-enhanced route itineraries for navigation of blind pedestrians in urban environment,Jan Balata and Zdenek Mikovec and Pavel Slavik,2018,9,Journal on Multimodal User Interfaces
10.1007/s12193-012-0094-8,A listener model: introducing personality traits,Elisabetta Bevacqua and Etienne de Sevin and Sylwia Julia Hyniewska and Catherine Pelachaud,2012,20,Journal on Multimodal User Interfaces
10.1007/s12193-019-00300-3,How do angry drivers respond to emotional music? A comprehensive perspective on assessing emotion,S. Maryam FakhrHosseini and Myounghoon Jeon,2019,17,Journal on Multimodal User Interfaces
10.1007/s12193-019-00301-2,Visualizing natural language interaction for conversational in-vehicle information systems to minimize driver distraction,Michael Braun and Nora Broy and Bastian Pfleging and Florian Alt,2019,23,Journal on Multimodal User Interfaces
10.1007/s12193-017-0256-9,Dynamic facial landmarking selection for emotion recognition using Gaussian processes,Hernán F. García and Mauricio A. Álvarez and Álvaro A. Orozco,2017,5,Journal on Multimodal User Interfaces
10.1007/s12193-012-0099-3,Conceptual frameworks for multimodal social signal processing,Paul M. Brunet and Roddy Cowie and Dirk Heylen and Anton Nijholt and Marc Schröder,2012,2,Journal on Multimodal User Interfaces
10.1007/s12193-020-00324-0,"Auditory displays and auditory user interfaces: art, design, science, and research",Myounghoon Jeon and Areti Andreopoulou and Brian F. G. Katz,2020,1,Journal on Multimodal User Interfaces
10.1007/s12193-019-00296-w,Anthropomorphising driver-truck interaction: a study on the current state of research and the introduction of two innovative concepts,Jana Fank and Natalie T. Richardson and Frank Diermeyer,2019,6,Journal on Multimodal User Interfaces
10.1007/s12193-012-0102-z,"Comments by words, face and body",Isabella Poggi and Francesca D’Errico and Laura Vincze,2013,16,Journal on Multimodal User Interfaces
10.1007/s12193-015-0185-4,It sounds real when you see it. Realistic sound source simulation in multimodal virtual environments,Ágoston Török and Daniel Mestre and Ferenc Honbolygó and Pierre Mallet and Jean-Marie Pergandi and Valéria Csépe,2015,4,Journal on Multimodal User Interfaces
10.1007/s12193-010-0035-3,An open source integrated framework for rapid prototyping of multimodal affective applications in digital entertainment,Diego Arnone and Alessandro Rossi and Massimo Bertoncini,2010,1,Journal on Multimodal User Interfaces
10.1007/s12193-010-0055-z,Cross-disciplinary approaches to multimodal user interfaces,Gualtiero Volpe and Antonio Camurri and Thierry Dutoit and Maurizio Mancini,2010,2,Journal on Multimodal User Interfaces
10.1007/s12193-010-0046-0,Bacteria Hunt,Christian Mühl and Hayrettin Gürkök and Danny Plass-Oude Bos and Marieke E. Thurlings and Lasse Scherffig and Matthieu Duvinage and Alexandra A. Elbakyan and SungWook Kang and Mannes Poel and Dirk Heylen,2010,61,Journal on Multimodal User Interfaces
10.1007/s12193-019-00310-1,Time Well Spent with multimodal mobile interactions,Nadia Elouali,2019,5,Journal on Multimodal User Interfaces
10.1007/s12193-014-0164-1,Perceptually salient haptic rendering for enhancing kinesthetic perception in virtual environments,Ravikiran B. Singapogu and Christopher C. Pagano and Timothy C. Burg and Paul G. Dorn and Ron Zacharia and DongBin Lee,2014,3,Journal on Multimodal User Interfaces
10.1007/s12193-013-0144-x,Analysis of significant dialog events in realistic human–computer interaction,Dmytro Prylipko and Dietmar Rösner and Ingo Siegert and Stephan Günther and Rafael Friesen and Matthias Haase and Bogdan Vlasenko and Andreas Wendemuth,2014,15,Journal on Multimodal User Interfaces
10.1007/s12193-015-0196-1,Synchronizing multimodal recordings using audio-to-audio alignment,Joren Six and Marc Leman,2015,5,Journal on Multimodal User Interfaces
10.1007/s12193-010-0047-z,Human movement expressivity for mobile active music listening,Maurizio Mancini and Giovanna Varni and Jari Kleimola and Gualtiero Volpe and Antonio Camurri,2010,1,Journal on Multimodal User Interfaces
10.1007/s12193-016-0228-5,Judging crowds’ size by ear and by eye in virtual reality,Marine Taffou and Jan Ondřej and Carol O’Sullivan and Olivier Warusfel and Isabelle Viaud-Delmon,2017,1,Journal on Multimodal User Interfaces
10.1007/s12193-010-0045-1,The challenges of engineering multimodal interaction,Marilyn McGee-Lennon and Laurence Nigay and Philip Gray,2010,1,Journal on Multimodal User Interfaces
10.1007/s12193-010-0054-0,Auditory visual prominence,Samer Al Moubayed and Jonas Beskow and Björn Granström,2009,13,Journal on Multimodal User Interfaces
10.1007/s12193-009-0023-7,Student mental state inference from unintentional body gestures using dynamic Bayesian networks,Abdul Rehman Abbasi and Matthew N. Dailey and Nitin V. Afzulpurkar and Takeaki Uno,2010,7,Journal on Multimodal User Interfaces
10.1007/s12193-021-00367-x,Predicting multimodal presentation skills based on instance weighting domain adaptation,Yutaro Yagi and Shogo Okada and Shota Shiobara and Sota Sugimura,2022,1,Journal on Multimodal User Interfaces
10.1007/s12193-014-0167-y,Does panel type affect haptic experience? An empirical comparison of touch screen panels for smartphones,Eunil Park and Ki Joon Kim and Jay Y. Ohm,2014,2,Journal on Multimodal User Interfaces
10.1007/s12193-008-0007-z,Speech and sliding text aided sign retrieval from hearing impaired sign news videos,Oya Aran and Ismail Ari and Lale Akarun and Erinc Dikici and Siddika Parlak and Murat Saraclar and Pavel Campr and Marek Hruz,2008,10,Journal on Multimodal User Interfaces
10.1007/bf02884428,Comparison between different feature extraction techniques for audio-visual speech recognition,Alin G. Chiţu and Leon J. M. Rothkrantz and Pascal Wiggers and Jacek C. Wojdel,2007,16,Journal on Multimodal User Interfaces
10.1007/s12193-014-0155-2,Special issue on multimodal interfaces in cognitive infocommunication systems,Peter Baranyi and Adam Csapo,2014,2,Journal on Multimodal User Interfaces
10.1007/s12193-011-0059-3,Automatic fingersign-to-speech translation system,Marek Hrúz and Pavel Campr and Erinç Dikici and Ahmet Alp Kındıroğlu and Zdeněk Krňoul and Alexander Ronzhin and Haşim Sak and Daniel Schorno and Hülya Yalçın and Lale Akarun and Oya Aran and Alexey Karpov and Murat Saraçlar and Milos Železný,2011,7,Journal on Multimodal User Interfaces
10.1007/s12193-017-0255-x,A framework for the descriptive specification of awareness support in multimodal user interfaces for collaborative activities,Jesús Gallardo and Crescencio Bravo and Ana Isabel Molina,2018,6,Journal on Multimodal User Interfaces
10.1007/s12193-020-00345-9,Circus in Motion: a multimodal exergame supporting vestibular therapy for children with autism,Oscar Peña and Franceli L. Cibrian and Monica Tentori,2021,4,Journal on Multimodal User Interfaces
10.1007/s12193-015-0191-6,Blind guide,Bálint Sövény and Gábor Kovács and Zsolt T. Kardkovács,2015,9,Journal on Multimodal User Interfaces
10.1007/s12193-010-0053-1,AVLaughterCycle,Jérôme Urbain and Radoslaw Niewiadomski and Elisabetta Bevacqua and Thierry Dutoit and Alexis Moinet and Catherine Pelachaud and Benjamin Picart and Joëlle Tilmanne and Johannes Wagner,2010,24,Journal on Multimodal User Interfaces
10.1007/s12193-010-0039-z,An investigation of user responses to specifically designed activities in a multimodal location based game,Lynne Baillie and Lee Morton and Stephen Uzor and David C. Moffatt,2010,4,Journal on Multimodal User Interfaces
10.1007/s12193-014-0166-z,Blind-environment interaction through voice augmented objects,Rosen Ivanov,2014,8,Journal on Multimodal User Interfaces
10.1007/s12193-015-0178-3,Pedestrian activity classification using patterns of motion and histogram of oriented gradient,Rifat Muhammad Mueid and Chandrama Ahmed and Md. Atiqur Rahman Ahad,2016,11,Journal on Multimodal User Interfaces
10.1007/s12193-022-00396-0,"Commanding a drone through body poses, improving the user experience",Brandon Yam-Viramontes and Héctor Cardona-Reyes and Javier González-Trejo and Cristian Trujillo-Espinoza and Diego Mercado-Ravell,2022,0,Journal on Multimodal User Interfaces
10.1007/s12193-018-0261-7,Real-time eye blink and wink detection for object selection in HCI systems,Hari Singh and Jaswinder Singh,2018,17,Journal on Multimodal User Interfaces
10.1007/s12193-020-00318-y,Mapping for meaning: the embodied sonification listening model and its implications for the mapping problem in sonic information design,Stephen Roddy and Brian Bridges,2020,7,Journal on Multimodal User Interfaces
10.1007/s12193-012-0109-5,A multi-modal dance corpus for research into interaction between humans in virtual environments,Slim Essid and Xinyu Lin and Marc Gowing and Georgios Kordelas and Anil Aksay and Philip Kelly and Thomas Fillon and Qianni Zhang and Alfred Dielmann and Vlado Kitanovski and Robin Tournemenne and Aymeric Masurelle and Ebroul Izquierdo and Noel E. O’Connor and Petros Daras and Gaël Richard,2012,6,Journal on Multimodal User Interfaces
10.1007/s12193-018-0283-1,Design and validation of an auditory biofeedback system for modification of running parameters,Valerio Lorenzoni and Pieter Van den Berghe and Pieter-Jan Maes and Tijl De Bie and Dirk De Clercq and Marc Leman,2019,13,Journal on Multimodal User Interfaces
10.1007/s12193-013-0129-9,Inter-rater reliability for emotion annotation in human–computer interaction: comparison and methodological improvements,Ingo Siegert and Ronald Böck and Andreas Wendemuth,2014,39,Journal on Multimodal User Interfaces
10.1007/s12193-014-0157-0,Social support agents for older adults: longitudinal affective computing in the home,Lazlo Ring and Lin Shi and Kathleen Totzke and Timothy Bickmore,2015,51,Journal on Multimodal User Interfaces
10.1007/s12193-010-0038-0,The activation of modality in virtual objects assembly,Guillaume Rivière and Nadine Couture and Patrick Reuter,2010,5,Journal on Multimodal User Interfaces
10.1007/s12193-022-00391-5,Designing multi-purpose devices to enhance users’ perception of haptics,Riccardo Galdieri and Cristian Camardella and Marcello Carrozzino and Antonio Frisoli,2022,0,Journal on Multimodal User Interfaces
10.1007/s12193-015-0208-1,Vanishing scares: biofeedback modulation of affective player experiences in a procedural horror game,Pedro A. Nogueira and Vasco Torres and Rui Rodrigues and Eugénio Oliveira and Lennart E. Nacke,2016,25,Journal on Multimodal User Interfaces
10.1007/s12193-009-0016-6,To talk or not to talk with a computer,Anton Batliner and Christian Hacker and Elmar Nöth,2008,12,Journal on Multimodal User Interfaces
10.1007/s12193-018-0264-4,Haptic feedback combined with movement sonification using a friction sound improves task performance in a virtual throwing task,Emma Frid and Jonas Moll and Roberto Bresin and Eva-Lotta Sallnäs Pysander,2019,9,Journal on Multimodal User Interfaces
10.1007/s12193-016-0234-7,Design and application of 2D illusory vibrotactile feedback for hand-held tablets,Youngsun Kim and Jaedong Lee and Gerard Jounghyun Kim,2017,5,Journal on Multimodal User Interfaces
10.1007/s12193-018-0268-0,An approach for exploring a video via multimodal feature extraction and user interactions,Fahim A. Salim and Fasih Haider and Owen Conlan and Saturnino Luz,2018,5,Journal on Multimodal User Interfaces
10.1007/s12193-015-0206-3,Integrity analysis of knee joint by acoustic emission technique,Tawhidul Islam Khan and Harino Yoho,2016,9,Journal on Multimodal User Interfaces
10.1007/s12193-016-0231-x,VR-based operating modes and metaphors for collaborative ergonomic design of industrial workstations,Huyen Nguyen and Charles Pontonnier and Simon Hilt and Thierry Duval and Georges Dumont,2017,10,Journal on Multimodal User Interfaces
10.1007/s12193-015-0176-5,Multimodal interaction with virtual worlds XMMVR: eXtensible language for MultiModal interaction with virtual reality worlds,Hector Olmedo and David Escudero and Valentín Cardeñoso,2015,5,Journal on Multimodal User Interfaces
10.1007/s12193-018-0260-8,Use of tactons to communicate a risk level through an enactive shoe,Landry Delphin Chapwouo Tchakouté and David Gagnon and Bob-Antoine Jerry Ménélas,2018,6,Journal on Multimodal User Interfaces
10.1007/s12193-008-0010-4,RAMCESS 2.X framework—expressive voice analysis for realtime and accurate synthesis of singing,Nicolas d‘Alessandro and Onur Babacan and Baris Bozkurt and Thomas Dubuisson and Andre Holzapfel and Loic Kessous and Alexis Moinet and Maxime Vlieghe,2008,3,Journal on Multimodal User Interfaces
10.1007/s12193-010-0043-3,Description languages for multimodal interaction: a set of guidelines and its illustration with SMUIML,Bruno Dumas and Denis Lalanne and Rolf Ingold,2010,23,Journal on Multimodal User Interfaces
10.1007/s12193-019-00293-z,Design and evaluation of a time adaptive multimodal virtual keyboard,Yogesh Kumar Meena and Hubert Cecotti and KongFatt Wong-Lin and Girijesh Prasad,2019,5,Journal on Multimodal User Interfaces
10.1007/s12193-013-0119-y,JVoiceXML as a modality component in the W3C multimodal architecture,Dirk Schnelle-Walka and Stefan Radomski and Max Mühlhäuser,2013,15,Journal on Multimodal User Interfaces
10.1007/s12193-015-0209-0,Hierarchical committee of deep convolutional neural networks for robust facial expression recognition,Bo-Kyeong Kim and Jihyeon Roh and Suh-Yeon Dong and Soo-Young Lee,2016,158,Journal on Multimodal User Interfaces
10.1007/s12193-013-0140-1,Annotation and interpretation of prosodic data in the HuComTech corpus for multimodal user interfaces,István Szekrényes,2014,5,Journal on Multimodal User Interfaces
10.1007/s12193-018-0267-1,Multimodal speech recognition: increasing accuracy using high speed video data,Denis Ivanko and Alexey Karpov and Dmitrii Fedotov and Irina Kipyatkova and Dmitry Ryumin and Dmitriy Ivanko and Wolfgang Minker and Milos Zelezny,2018,14,Journal on Multimodal User Interfaces
10.1007/s12193-020-00319-x,ECG sonification to support the diagnosis and monitoring of myocardial infarction,Andrea Lorena Aldana Blanco and Steffen Grautoff and Thomas Hermann,2020,7,Journal on Multimodal User Interfaces
10.1007/s12193-016-0221-z,Auditory navigation with a tubular acoustic model for interactive distance cues and personalized head-related transfer functions,Michele Geronazzo and Federico Avanzini and Federico Fontana,2016,6,Journal on Multimodal User Interfaces
10.1007/s12193-020-00322-2,Movement sonification expectancy model: leveraging musical expectancy theory to create movement-altering sonifications,Joseph Newbold and Nicolas E. Gold and Nadia Bianchi-Berthouze,2020,6,Journal on Multimodal User Interfaces
10.1007/s12193-015-0207-2,Audio-visual emotion recognition using multi-directional regression and Ridgelet transform,M. Shamim Hossain and Ghulam Muhammad,2016,58,Journal on Multimodal User Interfaces
10.1007/s12193-020-00362-8,PLAAN: Pain Level Assessment with Anomaly-detection based Network,Yi Li and Shreya Ghosh and Jyoti Joshi,2021,2,Journal on Multimodal User Interfaces
10.1007/s12193-019-00302-1,The role of respiration audio in multimodal analysis of movement qualities,Vincenzo Lussu and Radoslaw Niewiadomski and Gualtiero Volpe and Antonio Camurri,2020,5,Journal on Multimodal User Interfaces
10.1007/s12193-011-0077-1,Visual SceneMaker—a tool for authoring interactive virtual characters,Patrick Gebhard and Gregor Mehlmann and Michael Kipp,2012,37,Journal on Multimodal User Interfaces
10.1007/s12193-022-00390-6,Ipsilateral and contralateral warnings: effects on decision-making and eye movements in near-collision scenarios,Joost de Winter and Jimmy Hu and Bastiaan Petermeijer,2022,3,Journal on Multimodal User Interfaces
10.1007/s12193-016-0237-4,"The impact of human–robot multimodal communication on mental workload, usability preference, and expectations of robot behavior",Julian Abich and Daniel J. Barber,2017,10,Journal on Multimodal User Interfaces
10.1007/s12193-019-00315-w,A comparative assessment of Wi-Fi and acoustic signal-based HCI methods on the practicality,Hayoung Jeong and Taeho Kang and Jiwon Choi and Jong Kim,2020,0,Journal on Multimodal User Interfaces
10.1007/s12193-010-0050-4,Empirical investigation of the temporal relations between speech and facial expressions of emotion,Stéphanie Buisine and Yun Wang and Ouriel Grynszpan,2009,5,Journal on Multimodal User Interfaces
10.1007/s12193-019-00292-0,The contribution of force feedback to human performance in the teleoperation of multiple unmanned aerial vehicles,Hyoung Il Son,2019,3,Journal on Multimodal User Interfaces
10.1007/s12193-009-0030-8,Multimodal user’s affective state analysis in naturalistic interaction,George Caridakis and Kostas Karpouzis and Manolis Wallace and Loic Kessous and Noam Amir,2010,29,Journal on Multimodal User Interfaces
10.1007/s12193-019-00295-x,Multimodal interaction in automotive applications,Dirk Schnelle-Walka and David R. McGee and Bastian Pfleging,2019,2,Journal on Multimodal User Interfaces
10.1007/s12193-011-0083-3,Robotic Rabbit Companions: amusing or a nuisance?,Dirk Heylen and Betsy van Dijk and Anton Nijholt,2012,12,Journal on Multimodal User Interfaces
10.1007/s12193-010-0036-2,Distinctive aspects of mobile interaction and their implications for the design of multimodal interfaces,Luca Chittaro,2010,18,Journal on Multimodal User Interfaces
10.1007/s12193-017-0248-9,Thumb touch control range and usability factors of virtual keys for smartphone games,Hsinfu Huang and Ta-chun Huang,2019,2,Journal on Multimodal User Interfaces
10.1007/s12193-008-0006-0,MARQS: retrieving sketches learned from a single example using a dual-classifier,Brandon Paulson and Tracy Hammond,2008,8,Journal on Multimodal User Interfaces
10.1007/s12193-013-0121-4,A dataset for point of gaze detection using head poses and eye images,Christopher D. McMurrough and Vangelis Metsis and Dimitrios Kosmopoulos and Ilias Maglogiannis and Fillia Makedon,2013,11,Journal on Multimodal User Interfaces
10.1007/s12193-018-0273-3,Correction to: Haptic feedback combined with movement sonification using a friction sound improves task performance in a virtual throwing task,Emma Frid and Jonas Moll and Roberto Bresin and Eva-Lotta Sallnäs Pysander,2019,0,Journal on Multimodal User Interfaces
10.1007/s12193-020-00352-w,An audiovisual interface-based drumming system for multimodal human–robot interaction,Gökhan Ince and Rabia Yorganci and Ahmet Ozkul and Taha Berkay Duman and Hatice Köse,2021,2,Journal on Multimodal User Interfaces
10.1007/bf02910058,Similarity searching for on-line handwritten documents,Sascha Schimke and Claus Vielhauer,2007,4,Journal on Multimodal User Interfaces
10.1007/s12193-017-0239-x,Providing plasticity and redistribution for 3D user interfaces using the D3PART model,Jérémy Lacoche and Thierry Duval and Bruno Arnaldi and Eric Maisel and Jérôme Royan,2017,2,Journal on Multimodal User Interfaces
10.1007/s12193-016-0232-9,Automatic recognition of touch gestures in the corpus of social touch,Merel M. Jung and Mannes Poel and Ronald Poppe and Dirk K. J. Heylen,2017,23,Journal on Multimodal User Interfaces
10.1007/s12193-012-0115-7,Preface,Patrizia Paggio and n.m. n.m. and Dirk Heylen and Michael Kipp,2013,0,Journal on Multimodal User Interfaces
10.1007/s12193-013-0123-2,Multimodal assistive technologies for depression diagnosis and monitoring,Jyoti Joshi and Roland Goecke and Sharifa Alghowinem and Abhinav Dhall and Michael Wagner and Julien Epps and Gordon Parker and Michael Breakspear,2013,105,Journal on Multimodal User Interfaces
10.1007/s12193-011-0082-4,"Studying and enhancing talking condition recognition in stressful and emotional talking environments based on HMMs, CHMM2s and SPHMMs",Ismail Shahin,2012,21,Journal on Multimodal User Interfaces
10.1007/s12193-016-0212-0,Recognition of global hand gestures using self co-articulation information and classifier fusion,Joyeeta Singha and Rabul Hussain Laskar,2016,15,Journal on Multimodal User Interfaces
10.1007/s12193-012-0100-1,Preface,Elisabeth André and Marc Cavazza and Catherine Pelachaud,2012,0,Journal on Multimodal User Interfaces
10.1007/s12193-011-0075-3,Navigation of interactive sonifications and visualisations of time-series data using multi-touch computing,Sam Ferguson and Kirsty Beilharz and Claudia A. Calò,2012,3,Journal on Multimodal User Interfaces
10.1007/s12193-013-0125-0,User preferences for multi-device context-aware feedback in a digital coaching system,Randy Klaassen and Rieks op den Akker and Tine Lavrysen and Susan van Wissen,2013,13,Journal on Multimodal User Interfaces
10.1007/s12193-009-0015-7,Speech driven realistic mouth animation based on multi-modal unit selection,Dongmei Jiang and Ilse Ravyse and Hichem Sahli and Werner Verhelst,2008,6,Journal on Multimodal User Interfaces
10.1007/s12193-016-0215-x,Gesture recognition based on HMM-FNN model using a Kinect,Xiao-Li Guo and Ting-Ting Yang,2017,9,Journal on Multimodal User Interfaces
10.1007/s12193-017-0238-y,Cluster-based approach to discriminate the user’s state whether a user is embarrassed or thinking to an answer to a prompt,Yuya Chiba and Takashi Nose and Akinori Ito,2017,1,Journal on Multimodal User Interfaces
10.1007/s12193-018-0266-2,Experimenting with lipreading for large vocabulary continuous speech recognition,Karel Paleček,2018,5,Journal on Multimodal User Interfaces
10.1007/s12193-013-0147-7,A model for incremental grounding in spoken dialogue systems,Thomas Visser and David Traum and David DeVault and Rieks op den Akker,2014,6,Journal on Multimodal User Interfaces
10.1007/s12193-012-0098-4,Discrediting signals. A model of social evaluation to study discrediting moves in political debates,Francesca D’Errico and Isabella Poggi and Laura Vincze,2012,17,Journal on Multimodal User Interfaces
10.1007/s12193-020-00330-2,Sharing gaze rays for visual target identification tasks in collaborative augmented reality,Austin Erickson and Nahal Norouzi and Kangsoo Kim and Ryan Schubert and Jonathan Jules and Joseph J. LaViola and Gerd Bruder and Gregory F. Welch,2020,13,Journal on Multimodal User Interfaces
10.1007/s12193-019-00297-9,Investigating the effects of modality switches on driver distraction and interaction efficiency in the car,Florian Roider and Sonja Rümelin and Bastian Pfleging and Tom Gross,2019,6,Journal on Multimodal User Interfaces
10.1007/s12193-020-00333-z,Psychophysical comparison of the auditory and tactile perception: a survey,Sebastian Merchel and M. Ercan Altinsoy,2020,16,Journal on Multimodal User Interfaces
10.1007/s12193-015-0193-4,Nonverbal interaction contextualized in collaborative virtual environments,Adriana Peña Pérez Negrón and Nora Edith Rangel Bernal and Graciela Lara López,2015,6,Journal on Multimodal User Interfaces
10.1007/s12193-018-0282-2,Psychoacoustic auditory display for navigation: an auditory assistance system for spatial orientation tasks,Tim Ziemer and Holger Schultheis,2019,9,Journal on Multimodal User Interfaces
10.1007/s12193-013-0134-z,Improving awareness for 3D virtual collaboration by embedding the features of users’ physical environments and by augmenting interaction tools with cognitive feedback cues,Thierry Duval and Thi Thuong Huyen Nguyen and Cédric Fleury and Alain Chauffaut and Georges Dumont and Valérie Gouranton,2014,20,Journal on Multimodal User Interfaces
10.1007/s12193-020-00340-0,"Defining a vibrotactile toolkit for digital musical instruments: characterizing voice coil actuators, effects of loading, and equalization of the frequency response",Aditya Tirumala Bukkapatnam and Philippe Depalle and Marcelo M. Wanderley,2020,6,Journal on Multimodal User Interfaces
10.1007/s12193-016-0230-y,Multimodal feedback for teleoperation of multiple mobile robots in an outdoor environment,Ayoung Hong and Dong Gun Lee and Heinrich H. Bülthoff and Hyoung Il Son,2017,17,Journal on Multimodal User Interfaces
10.1007/s12193-012-0112-x,"A cross-cultural, multimodal, affective corpus for gesture expressivity analysis",G. Caridakis and J. Wagner and A. Raouzaiou and F. Lingenfelser and K. Karpouzis and E. Andre,2013,5,Journal on Multimodal User Interfaces
10.1007/s12193-009-0024-6,Natural interaction with a virtual guide in a virtual environment,Dennis Hofs and Mariët Theune and Rieks op den Akker,2010,8,Journal on Multimodal User Interfaces
10.1007/s12193-011-0081-5,Emotion-based interaction,Olga Sourina and Ling Li and Zhigeng Pan,2012,2,Journal on Multimodal User Interfaces
10.1007/s12193-010-0037-1,Tilt and go: exploring multimodal mobile maps in the field,Andrew Ramsay and Marilyn McGee-Lennon and Graham A. Wilson and Steven J. Gray and Philip Gray and François De Turenne,2010,6,Journal on Multimodal User Interfaces
10.1007/s12193-013-0135-y,Effect of stimulus intensity on response time distribution in multisensory integration,Ágoston Török and Orsolya Kolozsvári and Tamás Virágh and Ferenc Honbolygó and Valéria Csépe,2014,2,Journal on Multimodal User Interfaces
10.1007/s12193-008-0009-x,An audio-driven dancing avatar,Ferda Ofli and Yasemin Demir and Yücel Yemez and Engin Erzin and A. Murat Tekalp and Koray Balcı and İdil Kızoğlu and Lale Akarun and Cristian Canton-Ferrer and Joëlle Tilmanne and Elif Bozkurt and A. Tanju Erdem,2008,11,Journal on Multimodal User Interfaces
10.1007/s12193-020-00334-y,Multisensory instrumental dynamics as an emergent paradigm for digital musical creation,James Leonard and Jérôme Villeneuve and Alexandros Kontogeorgakopoulos,2020,2,Journal on Multimodal User Interfaces
10.1007/s12193-021-00383-x,The Audio-Corsi: an acoustic virtual reality-based technological solution for evaluating audio-spatial memory abilities,Walter Setti and Isaac Alonso-Martinez Engel and Luigi F. Cuturi and Monica Gori and Lorenzo Picinali,2022,2,Journal on Multimodal User Interfaces
10.1007/s12193-022-00394-2,TapCAPTCHA: non-visual CAPTCHA on touchscreens for visually impaired people,Mrim Alnfiai and Fawaz Alassery,2022,0,Journal on Multimodal User Interfaces
10.1007/s12193-018-0284-0,Does embodied training improve the recognition of mid-level expressive movement qualities sonification?,Radoslaw Niewiadomski and Maurizio Mancini and Andrea Cera and Stefano Piana and Corrado Canepa and Antonio Camurri,2019,7,Journal on Multimodal User Interfaces
10.1007/s12193-018-0274-2,Explorations in multiparty casual social talk and its relevance for social human machine dialogue,Emer Gilmartin and Benjamin R. Cowan and Carl Vogel and Nick Campbell,2018,7,Journal on Multimodal User Interfaces
10.1007/s12193-014-0154-3,Expressive non-verbal interaction in a string quartet: an analysis through head movements,Donald Glowinski and Floriane Dardard and Giorgio Gnecco and Stefano Piana and Antonio Camurri,2015,17,Journal on Multimodal User Interfaces
10.1007/s12193-019-00294-y,Personalised interactive sonification of musical performance data,KatieAnna E. Wolf and Rebecca Fiebrink,2019,4,Journal on Multimodal User Interfaces
10.1007/s12193-012-0105-9,"Head movements, facial expressions and feedback in conversations: empirical evidence from Danish multimodal data",Patrizia Paggio and Costanza Navarretta,2013,8,Journal on Multimodal User Interfaces
10.1007/s12193-011-0064-6,Eye-tracking based adaptive user interface: implicit human-computer interaction for preference indication,Shiwei Cheng and Ying Liu,2012,22,Journal on Multimodal User Interfaces
10.1007/s12193-015-0189-0,The recognition of acted interpersonal stance in police interrogations and the influence of actor proficiency,Merijn Bruijnes and Rieks op den Akker and Sophie Spitters and Merijn Sanders and Quihua Fu,2015,2,Journal on Multimodal User Interfaces
10.1007/s12193-015-0205-4,Examining brain activity while playing computer games,Anastasios G. Bakaoukas and Florin Coada and Fotis Liarokapis,2016,15,Journal on Multimodal User Interfaces
10.1007/s12193-016-0211-1,Automatic behavior analysis in tag games: from traditional spaces to interactive playgrounds,Alejandro Moreno and Ronald Poppe,2016,8,Journal on Multimodal User Interfaces
10.1007/s12193-020-00344-w,Haptic and audio interaction design,Thomas Pietrzak and Marcelo M. Wanderley,2020,2,Journal on Multimodal User Interfaces
10.1007/s12193-020-00359-3,Verbal empathy and explanation to encourage behaviour change intention,Amal Abdulrahman and Deborah Richards and Hedieh Ranjbartabar and Samuel Mascarenhas,2021,6,Journal on Multimodal User Interfaces
10.1007/s12193-018-0276-0,Communication via warm haptic interfaces does not increase social warmth,Christian J. A. M. Willemse and Dirk K. J. Heylen and Jan B. F. van Erp,2018,8,Journal on Multimodal User Interfaces
10.1007/s12193-013-0142-z,Predicting online lecture ratings based on gesturing and vocal behavior,Dong Seon Cheng and Hugues Salamin and Pietro Salvagnini and Marco Cristani and Alessandro Vinciarelli and Vittorio Murino,2014,10,Journal on Multimodal User Interfaces
10.1007/s12193-022-00398-y,Theory-based approach for assessing cognitive load during time-critical resource-managing human–computer interactions: an eye-tracking study,Natalia Sevcenko and Tobias Appel and Manuel Ninaus and Korbinian Moeller and Peter Gerjets,2023,1,Journal on Multimodal User Interfaces
10.1007/s12193-018-00291-7,Special issue editorial: Virtual Agents for Social Skills Training,Merijn Bruijnes and Jeroen Linssen and Dirk Heylen,2019,4,Journal on Multimodal User Interfaces
10.1007/s12193-012-0101-0,Emergent leaders through looking and speaking: from audio-visual data to multimodal recognition,Dairazalia Sanchez-Cortes and Oya Aran and Dinesh Babu Jayagopi and Marianne Schmid Mast and Daniel Gatica-Perez,2013,51,Journal on Multimodal User Interfaces
10.1007/s12193-014-0162-3,Haptic communication of dimensions of emotions using air jet based tactile stimulation,Mohamed Yassine Tsalamlal and Nizar Ouarti and Jean-Claude Martin and Mehdi Ammi,2015,22,Journal on Multimodal User Interfaces
10.1007/s12193-018-0281-3,Interactive sonification of U-depth images in a navigation aid for the visually impaired,Piotr Skulimowski and Mateusz Owczarek and Andrzej Radecki and Michal Bujacz and Dariusz Rzeszotarski and Pawel Strumillo,2019,18,Journal on Multimodal User Interfaces
10.1007/s12193-014-0169-9,Let’s talk! speaking virtual counselor offers you a brief intervention,Ugan Yasavur and Christine Lisetti and Naphtali Rishe,2014,25,Journal on Multimodal User Interfaces
10.1007/s12193-020-00332-0,“Let me explain!”: exploring the potential of virtual agents in explainable AI interaction design,Katharina Weitz and Dominik Schiller and Ruben Schlagowski and Tobias Huber and Elisabeth André,2021,25,Journal on Multimodal User Interfaces
10.1007/s12193-013-0133-0,Using unlabeled data to improve classification of emotional states in human computer interaction,Martin Schels and Markus Kächele and Michael Glodek and David Hrabal and Steffen Walter and Friedhelm Schwenker,2014,20,Journal on Multimodal User Interfaces
10.1007/s12193-014-0149-0,Combination of sequential class distributions from multiple channels using Markov fusion networks,Michael Glodek and Martin Schels and Friedhelm Schwenker and Günther Palm,2014,10,Journal on Multimodal User Interfaces
10.1007/s12193-014-0153-4,An analysis of player affect transitions in survival horror games,Vanus Vachiratamporn and Roberto Legaspi and Koichi Moriyama and Ken-ichi Fukui and Masayuki Numao,2015,20,Journal on Multimodal User Interfaces
10.1007/s12193-013-0130-3,An architecture for fluid real-time conversational agents: integrating incremental output generation and input processing,Stefan Kopp and Herwin van Welbergen and Ramin Yaghoubzadeh and Hendrik Buschmeier,2013,8,Journal on Multimodal User Interfaces
10.1007/s12193-020-00339-7,A BCI video game using neurofeedback improves the attention of children with autism,Jose Mercado and Lizbeth Escobedo and Monica Tentori,2021,13,Journal on Multimodal User Interfaces
10.1007/s12193-011-0080-6,Real-time EEG-based emotion recognition for music therapy,Olga Sourina and Yisi Liu and Minh Khoa Nguyen,2012,66,Journal on Multimodal User Interfaces
10.1007/s12193-015-0182-7,A survey of assistive technologies and applications for blind users on mobile platforms: a review and foundation for research,Ádám Csapó and György Wersényi and Hunor Nagy and Tony Stockman,2015,107,Journal on Multimodal User Interfaces
10.1007/s12193-018-0269-z,"Co-located augmented play-spaces: past, present, and perspectives",Robby van Delden and Steven Gerritsen and Dirk Heylen and Dennis Reidsma,2018,9,Journal on Multimodal User Interfaces
10.1007/s12193-016-0236-5,Diegetic user interfaces for virtual environments with HMDs: a user experience study with oculus rift,Paola Salomoni and Catia Prandi and Marco Roccetti and Lorenzo Casanova and Luca Marchetti and Gustavo Marfia,2017,37,Journal on Multimodal User Interfaces
10.1007/s12193-013-0122-3,"Induction, recording and recognition of natural emotions from facial expressions and speech prosody",Kostas Karpouzis and George Caridakis and Roddy Cowie and Ellen Douglas-Cowie,2013,4,Journal on Multimodal User Interfaces
10.1007/s12193-012-0095-7,Interactive sonification,Roberto Bresin and Thomas Hermann and Andy Hunt,2012,4,Journal on Multimodal User Interfaces
10.1007/s12193-015-0201-8,Preface,Péter Baranyi and Hassan Charaf and Anna Esposito and Péter Földesi and Helen Meng,2015,0,Journal on Multimodal User Interfaces
10.1007/s12193-021-00372-0,Behavior and usability analysis for multimodal user interfaces,Hamdi Dibeklioğlu and Elif Surer and Albert Ali Salah and Thierry Dutoit,2021,0,Journal on Multimodal User Interfaces
10.1007/s12193-018-0270-6,Three recent trends in Paralinguistics on the way to omniscient machine intelligence,Björn W. Schuller and Yue Zhang and Felix Weninger,2018,5,Journal on Multimodal User Interfaces
10.1007/bf02910056,ICANDO: Low cost multimodal interface for hand disabled people,Alexey Karpov and Andrey Ronzhin,2007,4,Journal on Multimodal User Interfaces
10.1007/bf02910055,Haptic access to conventional 2D maps for the visually impaired,Konstantinos Kostopoulos and Konstantinos Moustakas and Dimitrios Tzovaras and Giorgos Nikolakis and Céline Thillou and Bernard Gosselin,2007,11,Journal on Multimodal User Interfaces
10.1007/s12193-014-0170-3,Run-time model based framework for automatic evaluation of multimodal interfaces,Pedro Luis Mateo Navarro and Stefan Hillmann and Sebastian Möller and Diego Sevilla Ruiz and Gregorio Martínez Pérez,2014,3,Journal on Multimodal User Interfaces
10.1007/s12193-009-0018-4,Effect of speed difference between time-expanded speech and moving image of talker’s face on word intelligibility,Shuichi Sakamoto and Akihiro Tanaka and Komi Tsumura and Yôiti Suzuki,2008,2,Journal on Multimodal User Interfaces
10.1007/s12193-015-0188-1,KoalaPhone: touchscreen mobile phone UI for active seniors,Jan Balata and Zdenek Mikovec and Tomas Slavicek,2015,20,Journal on Multimodal User Interfaces
10.1007/s12193-022-00399-x,The effects of olfactory cues as Interface notifications on a mobile phone,Miao Huang and Chien-Hsiung Chen,2023,0,Journal on Multimodal User Interfaces
10.1007/s12193-022-00400-7,Virtual reality can mediate the learning phase of upper limb prostheses supporting a better-informed selection process,Lucas El Raghibi and Ange Pascal Muhoza and Jeanne Evrard and Hugo Ghazi and Grégoire van Oldeneel tot Oldenzeel and Victorien Sonneville and Benoît Macq and Renaud Ronsse,2023,0,Journal on Multimodal User Interfaces
10.1007/s12193-014-0172-1,Multi-modal natural interaction in game design: a comparative analysis of player experience in a large scale role-playing game,Pedro Alves Nogueira and Luís Filipe Teófilo and Pedro Brandão Silva,2015,3,Journal on Multimodal User Interfaces
10.1007/s12193-011-0057-5,Design and implementation of an affect-responsive interactive photo frame,Hamdi Dibeklioğlu and Marcos Ortega Hortas and Ilkka Kosunen and Petr Zuzánek and Albert Ali Salah and Theo Gevers,2011,7,Journal on Multimodal User Interfaces
10.1007/s12193-017-0253-z,Multimodal human-machine interface devices in the cloud,B. Estrany and C. Marin and M. Mascaró and A. Bibiloni and Y. Luo,2018,2,Journal on Multimodal User Interfaces
10.1007/s12193-008-0013-1,Guest Editorial of the special eNTERFACE issue,Bülent Sankur,2008,0,Journal on Multimodal User Interfaces
10.1007/s12193-017-0246-y,Vouch: multimodal touch-and-voice input for smart watches under difficult operating conditions,Jaedong Lee and Changhyeon Lee and Gerard Jounghyun Kim,2017,8,Journal on Multimodal User Interfaces
10.1007/s12193-014-0163-2,Best of affective computing and intelligent interaction 2013 in multimodal interactions,Mohammad Soleymani and Thierry Pun and Anton Nijholt,2015,1,Journal on Multimodal User Interfaces
10.1007/s12193-017-0247-x,Characterizing gesture knowledge transfer across multiple contexts of use,Radu-Daniel Vatavu,2017,8,Journal on Multimodal User Interfaces
10.1007/s12193-008-0001-5,Web based evaluation of proactive user interfaces,Daniel Schreiber and Melanie Hartmann and Felix Flentge and Max Mühlhäuser and Manuel Görtz and Thomas Ziegert,2008,1,Journal on Multimodal User Interfaces
10.1007/s12193-016-0218-7,Design and evaluation of the effectiveness of a sonification technique for real time heart-rate data,Benjamin Stahl and Balaji Thoshkahna,2016,6,Journal on Multimodal User Interfaces
10.1007/s12193-012-0117-5,Influencing factors on multimodal interaction during selection tasks,Felix Schüssel and Frank Honold and Michael Weber,2013,14,Journal on Multimodal User Interfaces
10.1007/s12193-014-0173-0,A Kinect-based 3D hand-gesture interface for 3D databases,Raul Herrera-Acuña and Vasileios Argyriou and Sergio A. Velastin,2015,5,Journal on Multimodal User Interfaces
10.1007/s12193-014-0148-1,"Non-visual identification, localization, and selection of entities of interest in a 3D environment",Bob-Antoine J. Menelas and Lorenzo Picinali and Patrick Bourdot and Brian F. G. Katz,2014,21,Journal on Multimodal User Interfaces
10.1007/s12193-011-0066-4,Sonophenology,Steven Ness and Paul Reimer and Justin Love and W. Andrew Schloss and George Tzanetakis,2012,0,Journal on Multimodal User Interfaces
10.1007/s12193-011-0076-2,Interactive sonification of expressive hand gestures on a handheld device,Marco Fabiani and Roberto Bresin and Gaël Dubus,2012,3,Journal on Multimodal User Interfaces
10.1007/s12193-019-00299-7,Investigating driver gaze behavior during lane changes using two visual cues: ambient light and focal icons,Andreas Löcken and Fei Yan and Wilko Heuten and Susanne Boll,2019,4,Journal on Multimodal User Interfaces
10.1007/s12193-021-00377-9,A wearable virtual touch system for IVIS in cars,Gowdham Prabhakar and Priyam Rajkhowa and Dharmesh Harsha and Pradipta Biswas,2022,1,Journal on Multimodal User Interfaces
10.1007/s12193-019-00313-y,Analysis of conversational listening skills toward agent-based social skills training,Hiroki Tanaka and Hidemi Iwasaka and Hideki Negoro and Satoshi Nakamura,2020,9,Journal on Multimodal User Interfaces
10.1007/s12193-011-0061-9,Enriching the user experience with multimodal interfaces,Albert Ali Salah and Thierry Dutoit,2011,0,Journal on Multimodal User Interfaces
10.1007/s12193-018-0265-3,A computational model for the emergence of turn-taking behaviors in user-agent interactions,Mathieu Jégou and Pierre Chevaillier,2018,1,Journal on Multimodal User Interfaces
10.1007/s12193-020-00341-z,Empirical evaluation and pathway modeling of visual attention to virtual humans in an appearance fidelity continuum,Matias Volonte and Reza Ghaiumy Anaraky and Rohith Venkatakrishnan and Roshan Venkatakrishnan and Bart P. Knijnenburg and Andrew T. Duchowski and Sabarish V. Babu,2021,2,Journal on Multimodal User Interfaces
10.1007/s12193-013-0118-z,AGNES: Connecting people in a multimodal way,Christian Peter and Andreas Kreiner and Martin Schröter and Hyosun Kim and Gerald Bieber and Fredrik Öhberg and Kei Hoshi and Eva L. Waterworth and John Waterworth and Soledad Ballesteros,2013,14,Journal on Multimodal User Interfaces
10.1007/s12193-014-0158-z,Gesture analysis in a case study with a tangible user interface for collaborative problem solving,Dimitra Anastasiou and Valérie Maquil and Eric Ras,2014,9,Journal on Multimodal User Interfaces
10.1007/bf02910054,A language perspective on the development of plastic multimodal user interfaces,Jean-Sébastien Sottet and Gaëlle Calvary and Joëlle Coutaz and Jean-Marie Favre and Jean Vanderdonckt and Adrian Stanciulescu and Sophie Lepreux,2007,4,Journal on Multimodal User Interfaces
10.1007/s12193-015-0187-2,Voice activity detection based on facial movement,Bart Joosten and Eric Postma and Emiel Krahmer,2015,8,Journal on Multimodal User Interfaces
10.1007/s12193-018-0262-6,Study on generic tangible objects used to collaborate remotely on RFID tabletops,Amira Bouabid and Sophie Lepreux and Christophe Kolski,2018,2,Journal on Multimodal User Interfaces
10.1007/s12193-020-00351-x,"Does an agent’s touch always matter? Study on virtual Midas touch, masculinity, social status, and compliance in Polish men",Justyna Świdrak and Grzegorz Pochwatko and Andrea Insabato,2021,3,Journal on Multimodal User Interfaces
10.1007/s12193-020-00357-5,Internet-based tailored virtual human health intervention to promote colorectal cancer screening: design guidelines from two user studies,Mohan Zalake and Fatemeh Tavassoli and Kyle Duke and Thomas George and Francois Modave and Jordan Neil and Janice Krieger and Benjamin Lok,2021,9,Journal on Multimodal User Interfaces
10.1007/s12193-015-0204-5,Video modeling and learning on Riemannian manifold for emotion recognition in the wild,Mengyi Liu and Ruiping Wang and Shaoxin Li and Zhiwu Huang and Shiguang Shan and Xilin Chen,2016,5,Journal on Multimodal User Interfaces
10.1007/s12193-021-00364-0,"MUMBAI: multi-person, multimodal board game affect and interaction analysis dataset",Metehan Doyran and Arjan Schimmel and Pınar Baki and Kübra Ergin and Batıkan Türkmen and Almıla Akdağ Salah and Sander C. J. Bakkes and Heysem Kaya and Ronald Poppe and Albert Ali Salah,2021,7,Journal on Multimodal User Interfaces
10.1007/s12193-019-00311-0,Sonification supports perception of brightness contrast,Niklas Rönnberg,2019,1,Journal on Multimodal User Interfaces
10.1007/s12193-020-00321-3,Interactive sonification strategies for the motion and emotion of dance performances,Steven Landry and Myounghoon Jeon,2020,8,Journal on Multimodal User Interfaces
10.1007/s12193-016-0222-y,SVM-based feature selection methods for emotion recognition from multimodal data,Cristian Torres-Valencia and Mauricio Álvarez-López and Álvaro Orozco-Gutiérrez,2017,36,Journal on Multimodal User Interfaces
10.1007/s12193-009-0033-5,Affect recognition for interactive companions: challenges and design in real world scenarios,Ginevra Castellano and Iolanda Leite and André Pereira and Carlos Martinho and Ana Paiva and Peter W. McOwan,2010,55,Journal on Multimodal User Interfaces
10.1007/s12193-021-00376-w,"SoundSight: a mobile sensory substitution device that sonifies colour, distance, and temperature",Giles Hamilton-Fletcher and James Alvarez and Marianna Obrist and Jamie Ward,2022,4,Journal on Multimodal User Interfaces
10.1007/s12193-019-00308-9,Multi-modal facial expression feature based on deep-neural networks,Wei Wei and Qingxuan Jia and Yongli Feng and Gang Chen and Ming Chu,2020,16,Journal on Multimodal User Interfaces
10.1007/s12193-021-00373-z,A gaze-based interactive system to explore artwork imagery,Piercarlo Dondi and Marco Porta and Angelo Donvito and Giovanni Volpe,2022,6,Journal on Multimodal User Interfaces
10.1007/s12193-018-0286-y,The sense of social agency in gaze leading,Samuel Recht and Ouriel Grynszpan,2019,7,Journal on Multimodal User Interfaces
10.1007/s12193-009-0032-6,On-line emotion recognition in a 3-D activation-valence-time continuum using acoustic and linguistic cues,Florian Eyben and Martin Wöllmer and Alex Graves and Björn Schuller and Ellen Douglas-Cowie and Roddy Cowie,2010,77,Journal on Multimodal User Interfaces
10.1007/s12193-010-0052-2,Non-verbal communication strategies to improve robustness in dialogue systems: a comparative study,David Pardo and Beatriz L. Mencia and Álvaro H. Trapote and Luis Hernández,2009,6,Journal on Multimodal User Interfaces
10.1007/s12193-020-00348-6,Developing a scenario-based video game generation framework for computer and virtual reality environments: a comparative usability study,Elif Surer and Mustafa Erkayaoğlu and Zeynep Nur Öztürk and Furkan Yücel and Emin Alp Bıyık and Burak Altan and Büşra Şenderin and Zeliha Oğuz and Servet Gürer and H. Şebnem Düzgün,2021,3,Journal on Multimodal User Interfaces
10.1007/s12193-009-0020-x,Emerging biometric modalities: a survey,Georgios Goudelis and Anastasios Tefas and Ioannis Pitas,2008,20,Journal on Multimodal User Interfaces
10.1007/s12193-017-0249-8,DecisionMind: revealing human cognition states in data analytics-driven decision making with a multimodal interface,Jianlong Zhou and Fang Chen,2018,6,Journal on Multimodal User Interfaces
10.1007/s12193-011-0087-z,Natural interaction with culturally adaptive virtual characters,Felix Kistler and Birgit Endrass and Ionut Damian and Chi Tai Dang and Elisabeth André,2012,42,Journal on Multimodal User Interfaces
10.1007/s12193-015-0179-2,Spatial and temporal variations of feature tracks for crowd behavior analysis,Hajer Fradi and Jean-Luc Dugelay,2016,17,Journal on Multimodal User Interfaces
10.1007/s12193-020-00358-4,Neighborhood based decision theoretic rough set under dynamic granulation for BCI motor imagery classification,K. Renuga Devi and H. Hannah Inbarani,2021,7,Journal on Multimodal User Interfaces
10.1007/s12193-015-0195-2,EmoNets: Multimodal deep learning approaches for emotion recognition in video,Samira Ebrahimi Kahou and Xavier Bouthillier and Pascal Lamblin and Caglar Gulcehre and Vincent Michalski and Kishore Konda and Sébastien Jean and Pierre Froumenty and Yann Dauphin and Nicolas Boulanger-Lewandowski and Raul Chandias Ferrari and Mehdi Mirza and David Warde-Farley and Aaron Courville and Pascal Vincent and Roland Memisevic and Christopher Pal and Yoshua Bengio,2016,221,Journal on Multimodal User Interfaces
10.1007/s12193-017-0250-2,Augmented 3D hands: a gesture-based mixed reality system for distributed collaboration,Weidong Huang and Leila Alem and Franco Tecchia and Henry Been-Lirn Duh,2018,46,Journal on Multimodal User Interfaces
10.1007/s12193-020-00335-x,The combination of visual communication cues in mixed reality remote collaboration,Seungwon Kim and Gun Lee and Mark Billinghurst and Weidong Huang,2020,22,Journal on Multimodal User Interfaces
10.1007/s12193-009-0021-9,The mental ingredients of bitterness,Isabella Poggi and Francesca D’Errico,2010,19,Journal on Multimodal User Interfaces
10.1007/s12193-020-00331-1,The effects of spatial auditory and visual cues on mixed reality remote collaboration,Jing Yang and Prasanth Sasikumar and Huidong Bai and Amit Barde and Gábor Sörös and Mark Billinghurst,2020,21,Journal on Multimodal User Interfaces
10.1007/s12193-015-0181-8,Mediated telemedicine vs. face-to-face medicine: efficiency in distress reduction,Almog Idan and Helene S. Wallach and Moshe Almagor and Yehezkel Waisman and Shai Linn,2015,9,Journal on Multimodal User Interfaces
10.1007/s12193-014-0165-0,A novel set of features for continuous hand gesture recognition,M. K. Bhuyan and D. Ajay Kumar and Karl F. MacDorman and Yuji Iwahori,2014,49,Journal on Multimodal User Interfaces
10.1007/s12193-008-0005-1,Prototyping and evaluating glove-based multimodal interfaces,Julián García and José P. Molina and Diego Martínez and Arturo S. García and Pascual González and Jean Vanderdonckt,2008,1,Journal on Multimodal User Interfaces
10.1007/s12193-018-0258-2,Model-based adaptive user interface based on context and user experience evaluation,Jamil Hussain and Anees Ul Hassan and Hafiz Syed Muhammad Bilal and Rahman Ali and Muhammad Afzal and Shujaat Hussain and Jaehun Bang and Oresti Banos and Sungyoung Lee,2018,56,Journal on Multimodal User Interfaces
10.1007/s12193-019-00306-x,Are older people any different from younger people in the way they want to interact with robots? Scenario based survey,Mriganka Biswas and Marta Romeo and Angelo Cangelosi and Ray B. Jones,2020,14,Journal on Multimodal User Interfaces
10.1007/s12193-020-00356-6,"Comparing mind perception in strategic exchanges: human-agent negotiation, dictator and ultimatum games",Minha Lee and Gale Lucas and Jonathan Gratch,2021,7,Journal on Multimodal User Interfaces
10.1007/s12193-015-0210-7,Emotion recognition in the wild via sparse transductive transfer linear discriminant analysis,Yuan Zong and Wenming Zheng and Xiaohua Huang and Keyu Yan and Jingwei Yan and Tong Zhang,2016,21,Journal on Multimodal User Interfaces
10.1007/s12193-009-0028-2,When my robot smiles at me: Enabling human-robot rapport via real-time head gesture mimicry,Laurel D. Riek and Philip C. Paul and Peter Robinson,2010,104,Journal on Multimodal User Interfaces
10.1007/s12193-013-0141-0,"Context-sensitive, cross-platform user interface generation",Miroslav Macik and Tomas Cerny and Pavel Slavik,2014,19,Journal on Multimodal User Interfaces
10.1007/s12193-021-00371-1,Training public speaking with virtual social interactions: effectiveness of real-time feedback and delayed feedback,Mathieu Chollet and Stacy Marsella and Stefan Scherer,2022,0,Journal on Multimodal User Interfaces
10.1007/s12193-021-00379-7,TapCalculator: nonvisual touchscreen calculator for visually impaired people preliminary user study,Mrim Alnfiai,2022,0,Journal on Multimodal User Interfaces
10.1007/s12193-022-00393-3,Exploring visual stimuli as a support for novices’ creative engagement with digital musical interfaces,Yongmeng Wu and Nick Bryan-Kinns and Jinyi Zhi,2022,0,Journal on Multimodal User Interfaces
10.1007/s12193-022-00392-4,A SLAM-based augmented reality app for the assessment of spatial short-term memory using visual and auditory stimuli,M.-Carmen Juan and Magdalena Mendez-Lopez and Camino Fidalgo and Ramon Molla and Roberto Vivo and David Paramo,2022,0,Journal on Multimodal User Interfaces
10.1007/s12193-022-00389-z,The multimodal EchoBorg: not as smart as it looks,Sara Falcone and Jan Kolkmeier and Merijn Bruijnes and Dirk Heylen,2022,0,Journal on Multimodal User Interfaces
10.1007/s12193-008-0011-3,Editorial,Jean Vanderdonckt,2008,0,Journal on Multimodal User Interfaces
10.1007/s12193-013-0127-y,Prediction of interface preferences with a classifier selection approach,Elena Vildjiounaite and Daniel Schreiber and Vesa Kyllönen and Marcus Ständer and Ilkka Niskanen and Jani Mäntyjärvi,2013,3,Journal on Multimodal User Interfaces
10.1007/s12193-021-00366-y,Grounding behaviours with conversational interfaces: effects of embodiment and failures,Dimosthenis Kontogiorgos and Andre Pereira and Joakim Gustafson,2021,2,Journal on Multimodal User Interfaces
10.1007/s12193-012-0114-8,A multimodal emotion corpus for Filipino and its uses,Jocelynn Cu and Katrina Ysabel Solomon and Merlin Teodosia Suarez and Madelene Sta. Maria,2013,2,Journal on Multimodal User Interfaces
10.1007/s12193-014-0156-1,Multi-robot behavior adaptation to local and global communication atmosphere in humans-robots interaction,Lue-Feng Chen and Zhen-Tao Liu and Min Wu and Fang-Yan Dong and Yoichi Yamazaki and Kaoru Hirota,2014,14,Journal on Multimodal User Interfaces
10.1007/s12193-020-00336-w,Tactile discrimination of material properties: application to virtual buttons for professional appliances,Yuri De Pra and Stefano Papetti and Federico Fontana and Hanna Järveläinen and Michele Simonato,2020,1,Journal on Multimodal User Interfaces
10.1007/s12193-015-0203-6,Combining feature-level and decision-level fusion in a hierarchical classifier for emotion recognition in the wild,Bo Sun and Liandong Li and Xuewen Wu and Tian Zuo and Ying Chen and Guoyan Zhou and Jun He and Xiaoming Zhu,2016,25,Journal on Multimodal User Interfaces
10.1007/s12193-020-00338-8,Guidelines for the design of a virtual patient for psychiatric interview training,Lucile Dupuy and Etienne de Sevin and Hélène Cassoudesalle and Orlane Ballot and Patrick Dehail and Bruno Aouizerate and Emmanuel Cuny and Jean-Arthur Micoulaud-Franchi and Pierre Philip,2021,2,Journal on Multimodal User Interfaces
10.1007/s12193-017-0257-8,Ultrasonic hand gesture recognition for mobile devices,Mohamed Saad and Chris J. Bleakley and Vivek Nigram and Paul Kettle,2018,10,Journal on Multimodal User Interfaces
10.1007/s12193-009-0031-7,Special issue on real-time affect analysis and interpretation: closing the affective loop in virtual agents and robots,Ginevra Castellano and Kostas Karpouzis and Christopher Peters and Jean-Claude Martin,2010,7,Journal on Multimodal User Interfaces
10.1007/bf02910057,A monocular system for person tracking: Implementation and testing,Georgios N. Stamou and Michail Krinidis and Nikos Nikolaidis and Ioannis Pitas,2007,4,Journal on Multimodal User Interfaces
10.1007/s12193-015-0192-5,Open issues in model transformations for multimodal applications,László Lengyel and Hassan Charaf,2015,0,Journal on Multimodal User Interfaces
10.1007/s12193-015-0200-9,Control of word processing environment using myoelectric signals,Antonín Pošusta and Adam J. Sporka and Ondřej Poláček and Šimon Rudolf and Jakub Otáhal,2015,1,Journal on Multimodal User Interfaces
10.1007/s12193-019-00303-0,Object acquisition and selection using automatic scanning and eye blinks in an HCI system,Hari Singh and Jaswinder Singh,2019,6,Journal on Multimodal User Interfaces
10.1007/s12193-020-00327-x,"Multimodal, visuo-haptic games for abstract theory instruction: grabbing charged particles",Felix G. Hamza-Lup and Ioana R. Goldbach,2021,4,Journal on Multimodal User Interfaces
10.1007/s12193-016-0233-8,Multimodal HCI: exploratory studies on effects of first impression and single modality ratings in retrospective evaluation,Benjamin Weiss and Ina Wechsung and Stefan Hillmann and Sebastian Möller,2017,4,Journal on Multimodal User Interfaces
10.1007/s12193-017-0245-z,Survey and implications for the design of new 3D audio production and authoring tools,Justin Dan Mathew and Stéphane Huot and Brian F. G. Katz,2017,3,Journal on Multimodal User Interfaces
10.1007/s12193-019-00312-z,Introduction to the special issue on interactive sonification,Jiajun Yang and Thomas Hermann and Roberto Bresin,2019,2,Journal on Multimodal User Interfaces
10.1007/s12193-020-00361-9,A novel focus encoding scheme for addressee detection in multiparty interaction using machine learning algorithms,Usman Malik and Mukesh Barange and Julien Saunier and Alexandre Pauchet,2021,0,Journal on Multimodal User Interfaces
10.1007/s12193-019-00316-9,Interactive gaze and finger controlled HUD for cars,Gowdham Prabhakar and Aparna Ramakrishnan and Modiksha Madan and L. R. D. Murthy and Vinay Krishna Sharma and Sachin Deshmukh and Pradipta Biswas,2020,20,Journal on Multimodal User Interfaces
10.1007/s12193-009-0019-3,A tactile option to reduce robot controller size,Elizabeth S. Redden and Linda R. Elliott and Rodger A. Pettitt and Christian B. Carstens,2008,1,Journal on Multimodal User Interfaces
10.1007/s12193-015-0184-5,A virtual reality system for strengthening awareness and participation in rehabilitation for post-stroke patients,Stefano Mottura and Luca Fontana and Sara Arlati and Andrea Zangiacomi and Claudia Redaelli and Marco Sacco,2015,11,Journal on Multimodal User Interfaces
10.1007/s12193-021-00365-z,Identifying and evaluating conceptual representations for auditory-enhanced interactive physics simulations,Brianna J. Tomlinson and Bruce N. Walker and Emily B. Moore,2021,0,Journal on Multimodal User Interfaces
10.1007/s12193-011-0073-5,A multimodal approach of generating 3D human-like talking agent,Minghao Yang and Jianhua Tao and Kaihui Mu and Ya Li and Jianfeng Che,2012,0,Journal on Multimodal User Interfaces
10.1007/s12193-015-0194-3,LMNN metric learning and fuzzy nearest neighbour classifier for hand gesture recognition,Tea Marasovic and Vladan Papic and Vlasta Zanchi,2015,1,Journal on Multimodal User Interfaces
10.1007/s12193-018-0285-z,SliceType: fast gaze typing with a merging keyboard,Burak Benligiray and Cihan Topal and Cuneyt Akinlar,2019,7,Journal on Multimodal User Interfaces
10.1007/s12193-016-0217-8,Audio-haptic interfaces for digital audio workstations,Oussama Metatla and Fiore Martin and Adam Parkinson and Nick Bryan-Kinns and Tony Stockman and Atau Tanaka,2016,14,Journal on Multimodal User Interfaces
10.1007/s12193-014-0161-4,Automatic nonverbal behavior indicators of depression and PTSD: the effect of gender,Giota Stratou and Stefan Scherer and Jonathan Gratch and Louis-Philippe Morency,2015,26,Journal on Multimodal User Interfaces
10.1007/s12193-017-0244-0,An ICA based head movement classification system using video signals,S. Devadethan and Geevarghese Titus,2017,1,Journal on Multimodal User Interfaces
10.1007/s12193-011-0068-2,Singing function,Florian Grond and Thomas Hermann,2012,1,Journal on Multimodal User Interfaces
10.1007/s12193-008-0012-2,Multimodal user interface for the communication of the disabled,Savvas Argyropoulos and Konstantinos Moustakas and Alexey A. Karpov and Oya Aran and Dimitrios Tzovaras and Thanos Tsakiris and Giovanna Varni and Byungjun Kwon,2008,7,Journal on Multimodal User Interfaces
10.1007/s12193-022-00397-z,Grouping and Determining Perceived Severity of Cyber-Attack Consequences: Gaining Information Needed to Sonify Cyber-Attacks,Keith S. Jones and Natalie R. Lodinger and Benjamin P. Widlus and Akbar Siami Namin and Emily Maw and Miriam Armstrong,2022,0,Journal on Multimodal User Interfaces
10.1007/s12193-019-00298-8,Design and evaluation of auditory-supported air gesture controls in vehicles,Jason Sterkenburg and Steven Landry and Myounghoon Jeon,2019,11,Journal on Multimodal User Interfaces
10.1007/s12193-012-0111-y,RAVEL: an annotated corpus for training robots with audiovisual abilities,Xavier Alameda-Pineda and Jordi Sanchez-Riera and Johannes Wienke and Vojtěch Franc and Jan Čech and Kaustubh Kulkarni and Antoine Deleforge and Radu Horaud,2013,15,Journal on Multimodal User Interfaces
10.1007/s12193-021-00369-9,Non-native speaker perception of Intelligent Virtual Agents in two languages: the impact of amount and type of grammatical mistakes,David Obremski and Jean-Luc Lugrin and Philipp Schaper and Birgit Lugrin,2021,6,Journal on Multimodal User Interfaces
10.1007/s12193-021-00370-2,RFID-based tangible and touch tabletop for dual reality in crisis management context,Walid Merrad and Alexis Héloir and Christophe Kolski and Antonio Krüger,2022,4,Journal on Multimodal User Interfaces
10.1007/s12193-013-0136-x,On the disambiguation of multifunctional discourse markers in multimodal interaction,Agnes Abuczki,2014,4,Journal on Multimodal User Interfaces
10.1007/s12193-013-0139-7,Real-time emergency response: improved management of real-time information during crisis situations,Jeffrey R. Blum and Alexander Eichhorn and Severin Smith and Michael Sterle-Contala and Jeremy R. Cooperstock,2014,19,Journal on Multimodal User Interfaces
10.1007/s12193-011-0063-7,Acoustic rendering of particle-based simulation of liquids in motion,Carlo Drioli and Davide Rocchesso,2012,5,Journal on Multimodal User Interfaces
10.1007/s12193-012-0106-8,Data-based analysis of speech and gesture: the Bielefeld Speech and Gesture Alignment corpus (SaGA) and its applications,Andy Lücking and Kirsten Bergman and Florian Hahn and Stefan Kopp and Hannes Rieser,2013,18,Journal on Multimodal User Interfaces
10.1007/s12193-014-0168-x,vDesign: a CAVE-based virtual design environment using hand interactions,Xiaoming Nan and Ziyang Zhang and Ning Zhang and Fei Guo and Yifeng He and Ling Guan,2014,7,Journal on Multimodal User Interfaces
10.1007/s12193-015-0190-7,Learning multimodal behavioral models for face-to-face social interaction,Alaeddine Mihoub and Gérard Bailly and Christian Wolf and Frédéric Elisei,2015,13,Journal on Multimodal User Interfaces
10.1007/s12193-011-0060-x,Continuous interaction with a virtual human,Dennis Reidsma and Iwan de Kok and Daniel Neiberg and Sathish Chandra Pammi and Bart van Straalen and Khiet Truong and Herwin van Welbergen,2011,21,Journal on Multimodal User Interfaces
10.1007/s12193-012-0104-x,A french corpus of audio and multimodal interactions in a health smart home,Anthony Fleury and Michel Vacher and François Portet and Pedro Chahuara and Norbert Noury,2013,13,Journal on Multimodal User Interfaces
10.1007/s12193-021-00380-0,Advanced multimodal interaction techniques and user interfaces for serious games and virtual environments,Fotis Liarokapis and Sebastian von Mammen and Athanasios Vourvopoulos,2021,2,Journal on Multimodal User Interfaces
10.1007/s12193-008-0004-2,Design and usability evaluation of multimodal interaction with finite state machines: a conceptual framework,Marie-Luce Bourguet and Jaeseung Chang,2008,4,Journal on Multimodal User Interfaces
10.1007/s12193-011-0067-3,Interactive hierarchy-based auditory displays for accessing and manipulating relational diagrams,Oussama Metatla and Nick Bryan-Kinns and Tony Stockman,2012,6,Journal on Multimodal User Interfaces
10.1007/s12193-016-0216-9,SONIGait: a wireless instrumented insole device for real-time sonification of gait,Brian Horsak and Ronald Dlapka and Michael Iber and Anna-Maria Gorgas and Anita Kiselka and Christian Gradl and Tarique Siragy and Jakob Doppler,2016,19,Journal on Multimodal User Interfaces
10.1007/s12193-017-0243-1,Consistent categorization of multimodal integration patterns during human–computer interaction,Roman Hak and Tomas Zeman,2017,5,Journal on Multimodal User Interfaces
10.1007/s12193-014-0150-7,An audio-visual dataset of human–human interactions in stressful situations,Iulia Lefter and Gertjan J. Burghouts and Leon J. M. Rothkrantz,2014,17,Journal on Multimodal User Interfaces
10.1007/s12193-013-0143-y,Recognizing signals of social attitude in interacting with Ambient Conversational Systems,Berardina De Carolis and Nicole Novielli,2014,5,Journal on Multimodal User Interfaces
10.1007/s12193-019-00307-w,Elderly users’ acceptance of mHealth user interface (UI) design-based culture: the moderator role of age,Ahmed Alsswey and Hosam Al-Samarraie,2020,33,Journal on Multimodal User Interfaces
10.1007/s12193-009-0025-5,"Multimodal emotion recognition in speech-based interaction using facial expression, body gesture and acoustic analysis",Loic Kessous and Ginevra Castellano and George Caridakis,2010,137,Journal on Multimodal User Interfaces
10.1007/s12193-020-00342-y,Developing a mobile activity game for stroke survivors—lessons learned,Charlotte Magnusson and Kirsten Rassmus-Gröhn and Bitte Rydeman,2020,2,Journal on Multimodal User Interfaces
10.1007/s12193-015-0174-7,Low complexity head tracking on portable android devices for real time message composition,Laura Montanini and Enea Cippitelli and Ennio Gambi and Susanna Spinsante,2015,7,Journal on Multimodal User Interfaces
10.1007/s12193-018-0287-x,Training the use of theory of mind using artificial agents,Kim Veltman and Harmen de Weerd and Rineke Verbrugge,2019,8,Journal on Multimodal User Interfaces
10.1007/s12193-016-0235-6,An insight into assistive technology for the visually impaired and blind people: state-of-the-art and future trends,Alexy Bhowmick and Shyamanta M. Hazarika,2017,125,Journal on Multimodal User Interfaces
